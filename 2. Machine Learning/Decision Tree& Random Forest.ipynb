{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree & Random Forest\n",
    "\n",
    "이 글은 인사이트 캠퍼스의 조성현 강사님의 강의를 수강하며 정리한 자료입니다. [조성현 강사님 블로그 바로가기](https://blog.naver.com/chunjein)\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "![image-20200718234831922](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- 직선 혹은 (초)평면으로 데이터를 분류하는 분류 알고리즘\n",
    "\n",
    "- 경계를 많이 사용하면 과잉적합의 우려가 있다. 따라서 정지기준, 사전/사후 가지치기 방법이 사용된다.\n",
    "  - 정지기준: 트리의 깊이 지정, 마지막 노드의 데이터 수 일정 수준이하로 떨어지지 않도록 지정\n",
    "  - 가지치기: 트리를 단순화시킨다.\n",
    "    - 사전 가지치기: 조기 정지규칙에 의해 트리 성장을 멈춤\n",
    "    - 사후 가지치기: 초기에 트리를 최대크기로 만들고, Trimming을 통해 복잡도를 줄여나간다.\n",
    "\n",
    "- 분류나 예측의 근거를 알 수 있으므로 쉽게 이해 가능\n",
    "\n",
    "- Feature마다 분류에 영향을 미치는 정도를 파악할 수 있다.\n",
    "\n",
    "  \n",
    "\n",
    "### 의사결정선 결정하기\n",
    "\n",
    "- 분할 전 부모 노드의 불순척도보다 분할 후 자식 노드의 불순척도가 낮을수록 좋다. -> 여러 분할을 시도하고 불순도가 가장 낮은 경계로 분할한다.\n",
    "\n",
    "- 불순척도\n",
    "\n",
    "  - 엔트로피(0~1의 값)\n",
    "    $$\n",
    "    H(t) = -\\sum\\limits_{i=1}^c{p(i|t)log_2p(i|t)}\n",
    "    $$\n",
    "\n",
    "  - 지니 지수(0~0.5의 값)\n",
    "    $$\n",
    "    Gini(t) = 1-\\sum\\limits_{i=1}^cp(i|t)^2\n",
    "    $$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Pre-pruning(사전 가지치기) - depth로 최적화\n",
    "\n",
    "- 트리의 깊이를 변화시켜가면서 test데이터의 정확도를 측정, 가장 높은 깊이를 찾아 최종 Decision Tree로 사용\n",
    "\n",
    "```python\n",
    "for k in range(1,20):\n",
    "    dt = DecisionTreeClassifier(criterion = 'gini', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainGini.append((dt.predict(trainX)==trainY).mean())\n",
    "    testGini.append((dt.predict(testX)==testY).mean())\n",
    "    \n",
    "    dt = DecisionTreeClassifier(criterion ='entropy', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainEntropy.append(dt.score(trainX,trainY))\n",
    "    testEntropy.append(dt.score(testX,testY))\n",
    "```\n",
    "\n",
    "![image-20200719074811654](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- test dataset으로 측정한 정확도가 낮아지는 k=7 근처로 트리의 깊이를 지정한다.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Post-pruning(사후 가지치기) - α로 최적화\n",
    "\n",
    "- 최대한 복잡하게 트리를 구성한 뒤, 에러가 작아지는 방향으로 가지치기를 수행.\n",
    "\n",
    "- $$\n",
    "  오분류율: e_a(T) = \\frac{e+N*α}D\n",
    "  $$\n",
    "\n",
    "  - **<u>오분류율에 패널티항(α)을 추가하고 leaf-node가 많아질 수록 패널티를 크게 부여한다.</u>**\n",
    "    - e: 잘못 분류된 개수(leaf-node의 숫자가 작은 것)\n",
    "    - N: leaf-node개수\n",
    "    - D: data 개수\n",
    "\n",
    "- 즉, α=0부터 무한대까지 관찰하며 최적의 알파를 찾는다.\n",
    "\n",
    "```python\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(trainX, trainY)\n",
    "#이 함수는 학습데이터에 대해서 내부적으로 알파값을 변화시켜가면서 트리를 만들고 에러를 측정한다.\n",
    "#path에 impurities(불순도)와 alpha(패널티)가 리턴된다.\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "path\n",
    ">>>\n",
    "{'ccp_alphas': array([ 0.00000000e+00, -2.71050543e-20,  0.00000000e+00, ...,\n",
    "         1.56445662e-02,  2.65423192e-02,  6.05744854e-02]),\n",
    " 'impurities': array([0.03445308, 0.03445308, 0.03445308, ..., 0.28040352, 0.30694584,\n",
    "        0.36752033])}\n",
    "\n",
    "clfs = []\n",
    "for i, ccp_alpha in enumerate(ccp_alphas):\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(trainX, trainY)\n",
    "    clfs.append(clf)\n",
    "        \n",
    "    print('%d) ccp_alphas = %.4f done.' % (i, ccp_alpha))\n",
    "\n",
    "print('마지막 tree의 노드 개수 = %d' % clfs[-1].tree_.node_count)\n",
    "print('마지막 tree의 alpha = %.4f' % ccp_alphas[-1])\n",
    "print('마지막 tree의 depth = %d' % clfs[-1].tree_.max_depth)\n",
    "\n",
    ">>>\n",
    "마지막 tree의 노드 개수 = 3\n",
    "마지막 tree의 alpha = 0.0265\n",
    "마지막 tree의 depth = 1\n",
    "\n",
    "# clfs는 앞 부분 n개만 사용한다. 뒷 부분은 alpha가 너무 크기 때문에 제외한다.\n",
    "# test_scores[:n]중 가장 큰 최적 alpha를 찾는다.\n",
    "opt_alpha = ccp_alphas[np.argmax(test_scores[:n])]\n",
    "\n",
    "# opt_alpha를 적용한 tree를 사용한다.\n",
    "dt = DecisionTreeClassifier(ccp_alpha=opt_alpha)\n",
    "dt.fit(trainX, trainY)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Feature importrance\n",
    "\n",
    "- Decision Tree는 하위 tree의 양쪽 불순도가 최소가 되도록 분기를 결정한다. 분기를 완료한 뒤, feature별로 얼마나 불순도를 감소시켰는지 평균감소율을 계산해 feature들의 중요도를 판단할 수 있다.\n",
    "\n",
    "```python\n",
    "feature_importance = dt.feature_importances_\n",
    "feature_name = list(income.columns)\n",
    "n_feature = trainX.shape[1]\n",
    "idx = np.arange(n_feature)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- 중복허용, 균등분포로 n개의 서브 데이터를 샘플링하고, 각 서브 데이터마다 Decision Tree를 구축한다.\n",
    "- 서브 데이터는 랜덤하게 추출되므로 서브트리들은 약간씩 다르게 구성된다.\n",
    "    - 분산  감소, 일반화 특성 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "학습데이터의 정확도 = 1.00\n",
      "시험데이터의 정확도 = 0.93\n",
      "\n",
      "Confusion matrix :\n",
      "[[11  0  0]\n",
      " [ 0  5  0]\n",
      " [ 0  2 12]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        11\n",
      "  versicolor       0.71      1.00      0.83         5\n",
      "   virginica       1.00      0.86      0.92        14\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.90      0.95      0.92        30\n",
      "weighted avg       0.95      0.93      0.94        30\n",
      "\n",
      "\n",
      "Subtree별 시험데이터 정확도 :\n",
      "subtree (0) = 0.97\n",
      "subtree (1) = 0.97\n",
      "subtree (2) = 0.93\n",
      "subtree (3) = 0.97\n",
      "subtree (4) = 0.97\n",
      "subtree (5) = 0.93\n",
      "subtree (6) = 0.83\n",
      "subtree (7) = 0.93\n",
      "subtree (8) = 0.93\n",
      "subtree (9) = 0.93\n",
      "subtree (10) = 0.97\n",
      "subtree (11) = 0.90\n",
      "subtree (12) = 0.90\n",
      "subtree (13) = 0.93\n",
      "subtree (14) = 1.00\n",
      "subtree (15) = 1.00\n",
      "subtree (16) = 0.97\n",
      "subtree (17) = 1.00\n",
      "subtree (18) = 0.97\n",
      "subtree (19) = 0.97\n",
      "subtree (20) = 0.93\n",
      "subtree (21) = 1.00\n",
      "subtree (22) = 1.00\n",
      "subtree (23) = 0.97\n",
      "subtree (24) = 0.90\n",
      "subtree (25) = 0.83\n",
      "subtree (26) = 0.93\n",
      "subtree (27) = 0.93\n",
      "subtree (28) = 0.93\n",
      "subtree (29) = 0.93\n",
      "subtree (30) = 0.83\n",
      "subtree (31) = 1.00\n",
      "subtree (32) = 0.93\n",
      "subtree (33) = 0.93\n",
      "subtree (34) = 0.87\n",
      "subtree (35) = 0.97\n",
      "subtree (36) = 0.90\n",
      "subtree (37) = 0.97\n",
      "subtree (38) = 0.97\n",
      "subtree (39) = 1.00\n",
      "subtree (40) = 0.93\n",
      "subtree (41) = 0.97\n",
      "subtree (42) = 0.97\n",
      "subtree (43) = 0.93\n",
      "subtree (44) = 0.93\n",
      "subtree (45) = 0.90\n",
      "subtree (46) = 1.00\n",
      "subtree (47) = 0.97\n",
      "subtree (48) = 0.90\n",
      "subtree (49) = 0.93\n",
      "subtree (50) = 0.97\n",
      "subtree (51) = 0.93\n",
      "subtree (52) = 0.97\n",
      "subtree (53) = 0.93\n",
      "subtree (54) = 0.93\n",
      "subtree (55) = 0.80\n",
      "subtree (56) = 1.00\n",
      "subtree (57) = 0.97\n",
      "subtree (58) = 1.00\n",
      "subtree (59) = 0.90\n",
      "subtree (60) = 0.97\n",
      "subtree (61) = 1.00\n",
      "subtree (62) = 0.97\n",
      "subtree (63) = 0.93\n",
      "subtree (64) = 0.97\n",
      "subtree (65) = 0.97\n",
      "subtree (66) = 1.00\n",
      "subtree (67) = 1.00\n",
      "subtree (68) = 0.97\n",
      "subtree (69) = 0.93\n",
      "subtree (70) = 0.97\n",
      "subtree (71) = 1.00\n",
      "subtree (72) = 0.90\n",
      "subtree (73) = 0.93\n",
      "subtree (74) = 0.93\n",
      "subtree (75) = 1.00\n",
      "subtree (76) = 0.97\n",
      "subtree (77) = 0.97\n",
      "subtree (78) = 0.93\n",
      "subtree (79) = 0.93\n",
      "subtree (80) = 0.93\n",
      "subtree (81) = 1.00\n",
      "subtree (82) = 0.93\n",
      "subtree (83) = 0.93\n",
      "subtree (84) = 0.93\n",
      "subtree (85) = 0.90\n",
      "subtree (86) = 0.93\n",
      "subtree (87) = 0.97\n",
      "subtree (88) = 0.90\n",
      "subtree (89) = 0.97\n",
      "subtree (90) = 0.97\n",
      "subtree (91) = 0.93\n",
      "subtree (92) = 0.97\n",
      "subtree (93) = 0.97\n",
      "subtree (94) = 0.93\n",
      "subtree (95) = 1.00\n",
      "subtree (96) = 0.87\n",
      "subtree (97) = 1.00\n",
      "subtree (98) = 0.97\n",
      "subtree (99) = 0.97\n",
      "\n",
      "class-0 precision : 1.00\n",
      "class-1 precision : 0.71\n",
      "class-2 precision : 1.00\n",
      "\n",
      "class-0 recall : 1.00\n",
      "class-1 recall : 1.00\n",
      "class-2 recall : 0.86\n",
      "\n",
      "class-0 f1-score : 1.00\n",
      "class-1 f1-score : 0.83\n",
      "class-2 f1-score : 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# iris 데이터를 읽어온다.\n",
    "iris = load_iris()\n",
    "\n",
    "# Train 데이터 세트와 Test 데이터 세트를 구성한다\n",
    "trainX, testX, trainY, testY = \\\n",
    "    train_test_split(iris['data'], iris['target'], test_size = 0.2)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "rf.fit(trainX, trainY)\n",
    "\n",
    "# 학습데이터와 시험데이터의 정확도를 측정한다.\n",
    "print('\\n학습데이터의 정확도 = %.2f' % rf.score(trainX, trainY))\n",
    "print('시험데이터의 정확도 = %.2f' % rf.score(testX, testY))\n",
    "\n",
    "# 시험데이터의 confusion matrix를 작성한다 (row : actual, col : predict)\n",
    "predY = rf.predict(testX)\n",
    "print('\\nConfusion matrix :')\n",
    "print(confusion_matrix(testY, predY))\n",
    "print()\n",
    "print(classification_report(testY, predY, target_names=iris.target_names))\n",
    "\n",
    "# Sub tree별 시험데이터의 정확도를 확인한다.\n",
    "print('\\nSubtree별 시험데이터 정확도 :')\n",
    "for i in range(100):\n",
    "    subTree = rf.estimators_[i]\n",
    "    print('subtree (%d) = %.2f' % (i, subTree.score(testX, testY)))\n",
    "\n",
    "# classification_report()를 해석해 본다.\n",
    "import numpy as np\n",
    "label = np.vstack([testY, predY]).T\n",
    "\n",
    "# class = n이라고 예측한 것 중 실제 classe=n인 비율\n",
    "def precision(n):\n",
    "    y = label[label[:, 1] == n]\n",
    "    match = y[y[:, 0] == y[:, 1]]\n",
    "    return match.shape[0] / y.shape[0]\n",
    "\n",
    "print()\n",
    "print('class-0 precision : %.2f' % precision(0))\n",
    "print('class-1 precision : %.2f' % precision(1))\n",
    "print('class-2 precision : %.2f' % precision(2))\n",
    "\n",
    "# 실제 class = n인 것중 classe=n으로 예측한 비율\n",
    "def recall(n):\n",
    "    y = label[label[:, 0] == n]\n",
    "    match = y[y[:, 0] == y[:, 1]]\n",
    "    return match.shape[0] / y.shape[0]\n",
    "\n",
    "print()\n",
    "print('class-0 recall : %.2f' % recall(0))\n",
    "print('class-1 recall : %.2f' % recall(1))\n",
    "print('class-2 recall : %.2f' % recall(2))\n",
    "\n",
    "# F1-score\n",
    "def f1_score(n):\n",
    "    p = precision(n)\n",
    "    r = recall(n)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "print()\n",
    "print('class-0 f1-score : %.2f' % f1_score(0))\n",
    "print('class-1 f1-score : %.2f' % f1_score(1))\n",
    "print('class-2 f1-score : %.2f' % f1_score(2))"
   ]
  },
  {
   "attachments": {
    "image_check.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image_i](attachment:image_check.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
