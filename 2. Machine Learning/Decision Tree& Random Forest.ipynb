{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree & Random Forest\n",
    "\n",
    "이 글은 인사이트 캠퍼스의 조성현 강사님의 강의를 수강하며 정리한 자료입니다. [조성현 강사님 블로그 바로가기](https://blog.naver.com/chunjein)\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "![image-20200718234831922](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- 직선 혹은 (초)평면으로 데이터를 분류하는 분류 알고리즘\n",
    "\n",
    "- 경계를 많이 사용하면 과잉적합의 우려가 있다. 따라서 정지기준, 사전/사후 가지치기 방법이 사용된다.\n",
    "  - 정지기준: 트리의 깊이 지정, 마지막 노드의 데이터 수 일정 수준이하로 떨어지지 않도록 지정\n",
    "  - 가지치기: 트리를 단순화시킨다.\n",
    "    - 사전 가지치기: 조기 정지규칙에 의해 트리 성장을 멈춤\n",
    "    - 사후 가지치기: 초기에 트리를 최대크기로 만들고, Trimming을 통해 복잡도를 줄여나간다.\n",
    "\n",
    "- 분류나 예측의 근거를 알 수 있으므로 쉽게 이해 가능\n",
    "\n",
    "- Feature마다 분류에 영향을 미치는 정도를 파악할 수 있다.\n",
    "\n",
    "  \n",
    "\n",
    "### 의사결정선 결정하기\n",
    "\n",
    "- 분할 전 부모 노드의 불순척도보다 분할 후 자식 노드의 불순척도가 낮을수록 좋다. -> 여러 분할을 시도하고 불순도가 가장 낮은 경계로 분할한다.\n",
    "\n",
    "- 불순척도\n",
    "\n",
    "  - 엔트로피(0~1의 값)\n",
    "    $$\n",
    "    H(t) = -\\sum\\limits_{i=1}^c{p(i|t)log_2p(i|t)}\n",
    "    $$\n",
    "\n",
    "  - 지니 지수(0~0.5의 값)\n",
    "    $$\n",
    "    Gini(t) = 1-\\sum\\limits_{i=1}^cp(i|t)^2\n",
    "    $$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Pre-pruning(사전 가지치기) - depth로 최적화\n",
    "\n",
    "- 트리의 깊이를 변화시켜가면서 test데이터의 정확도를 측정, 가장 높은 깊이를 찾아 최종 Decision Tree로 사용\n",
    "\n",
    "```python\n",
    "for k in range(1,20):\n",
    "    dt = DecisionTreeClassifier(criterion = 'gini', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainGini.append((dt.predict(trainX)==trainY).mean())\n",
    "    testGini.append((dt.predict(testX)==testY).mean())\n",
    "    \n",
    "    dt = DecisionTreeClassifier(criterion ='entropy', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainEntropy.append(dt.score(trainX,trainY))\n",
    "    testEntropy.append(dt.score(testX,testY))\n",
    "```\n",
    "\n",
    "![image-20200719074811654](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- test dataset으로 측정한 정확도가 낮아지는 k=7 근처로 트리의 깊이를 지정한다.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Post-pruning(사후 가지치기) - α로 최적화\n",
    "\n",
    "- 최대한 복잡하게 트리를 구성한 뒤, 에러가 작아지는 방향으로 가지치기를 수행.\n",
    "\n",
    "- $$\n",
    "  오분류율: e_a(T) = \\frac{e+N*α}D\n",
    "  $$\n",
    "\n",
    "  - **<u>오분류율에 패널티항(α)을 추가하고 leaf-node가 많아질 수록 패널티를 크게 부여한다.</u>**\n",
    "    - e: 잘못 분류된 개수(leaf-node의 숫자가 작은 것)\n",
    "    - N: leaf-node개수\n",
    "    - D: data 개수\n",
    "\n",
    "- 즉, α=0부터 무한대까지 관찰하며 최적의 알파를 찾는다.\n",
    "\n",
    "```python\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(trainX, trainY)\n",
    "#이 함수는 학습데이터에 대해서 내부적으로 알파값을 변화시켜가면서 트리를 만들고 에러를 측정한다.\n",
    "#path에 impurities(불순도)와 alpha(패널티)가 리턴된다.\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "path\n",
    ">>>\n",
    "{'ccp_alphas': array([ 0.00000000e+00, -2.71050543e-20,  0.00000000e+00, ...,\n",
    "         1.56445662e-02,  2.65423192e-02,  6.05744854e-02]),\n",
    " 'impurities': array([0.03445308, 0.03445308, 0.03445308, ..., 0.28040352, 0.30694584,\n",
    "        0.36752033])}\n",
    "\n",
    "clfs = []\n",
    "for i, ccp_alpha in enumerate(ccp_alphas):\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(trainX, trainY)\n",
    "    clfs.append(clf)\n",
    "        \n",
    "    print('%d) ccp_alphas = %.4f done.' % (i, ccp_alpha))\n",
    "\n",
    "print('마지막 tree의 노드 개수 = %d' % clfs[-1].tree_.node_count)\n",
    "print('마지막 tree의 alpha = %.4f' % ccp_alphas[-1])\n",
    "print('마지막 tree의 depth = %d' % clfs[-1].tree_.max_depth)\n",
    "\n",
    ">>>\n",
    "마지막 tree의 노드 개수 = 3\n",
    "마지막 tree의 alpha = 0.0265\n",
    "마지막 tree의 depth = 1\n",
    "\n",
    "# clfs는 앞 부분 n개만 사용한다. 뒷 부분은 alpha가 너무 크기 때문에 제외한다.\n",
    "# test_scores[:n]중 가장 큰 최적 alpha를 찾는다.\n",
    "opt_alpha = ccp_alphas[np.argmax(test_scores[:n])]\n",
    "\n",
    "# opt_alpha를 적용한 tree를 사용한다.\n",
    "dt = DecisionTreeClassifier(ccp_alpha=opt_alpha)\n",
    "dt.fit(trainX, trainY)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Feature importrance\n",
    "\n",
    "- Decision Tree는 하위 tree의 양쪽 불순도가 최소가 되도록 분기를 결정한다. 분기를 완료한 뒤, feature별로 얼마나 불순도를 감소시켰는지 평균감소율을 계산해 feature들의 중요도를 판단할 수 있다.\n",
    "\n",
    "```python\n",
    "feature_importance = dt.feature_importances_\n",
    "feature_name = list(income.columns)\n",
    "n_feature = trainX.shape[1]\n",
    "idx = np.arange(n_feature)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- 중복허용, 균등분포로 n개의 서브 데이터를 샘플링하고, 각 서브 데이터마다 Decision Tree를 구축한다.\n",
    "- 서브 데이터는 랜덤하게 추출되므로 서브트리들은 약간씩 다르게 구성된다.\n",
    "    - 분산  감소, 일반화 특성 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "학습데이터의 정확도 = 1.00\n",
      "시험데이터의 정확도 = 0.93\n",
      "\n",
      "Confusion matrix :\n",
      "[[11  0  0]\n",
      " [ 0  5  0]\n",
      " [ 0  2 12]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        11\n",
      "  versicolor       0.71      1.00      0.83         5\n",
      "   virginica       1.00      0.86      0.92        14\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.90      0.95      0.92        30\n",
      "weighted avg       0.95      0.93      0.94        30\n",
      "\n",
      "\n",
      "Subtree별 시험데이터 정확도 :\n",
      "subtree (0) = 0.97\n",
      "subtree (1) = 0.97\n",
      "subtree (2) = 0.93\n",
      "subtree (3) = 0.97\n",
      "subtree (4) = 0.97\n",
      "subtree (5) = 0.93\n",
      "subtree (6) = 0.83\n",
      "subtree (7) = 0.93\n",
      "subtree (8) = 0.93\n",
      "subtree (9) = 0.93\n",
      "subtree (10) = 0.97\n",
      "subtree (11) = 0.90\n",
      "subtree (12) = 0.90\n",
      "subtree (13) = 0.93\n",
      "subtree (14) = 1.00\n",
      "subtree (15) = 1.00\n",
      "subtree (16) = 0.97\n",
      "subtree (17) = 1.00\n",
      "subtree (18) = 0.97\n",
      "subtree (19) = 0.97\n",
      "subtree (20) = 0.93\n",
      "subtree (21) = 1.00\n",
      "subtree (22) = 1.00\n",
      "subtree (23) = 0.97\n",
      "subtree (24) = 0.90\n",
      "subtree (25) = 0.83\n",
      "subtree (26) = 0.93\n",
      "subtree (27) = 0.93\n",
      "subtree (28) = 0.93\n",
      "subtree (29) = 0.93\n",
      "subtree (30) = 0.83\n",
      "subtree (31) = 1.00\n",
      "subtree (32) = 0.93\n",
      "subtree (33) = 0.93\n",
      "subtree (34) = 0.87\n",
      "subtree (35) = 0.97\n",
      "subtree (36) = 0.90\n",
      "subtree (37) = 0.97\n",
      "subtree (38) = 0.97\n",
      "subtree (39) = 1.00\n",
      "subtree (40) = 0.93\n",
      "subtree (41) = 0.97\n",
      "subtree (42) = 0.97\n",
      "subtree (43) = 0.93\n",
      "subtree (44) = 0.93\n",
      "subtree (45) = 0.90\n",
      "subtree (46) = 1.00\n",
      "subtree (47) = 0.97\n",
      "subtree (48) = 0.90\n",
      "subtree (49) = 0.93\n",
      "subtree (50) = 0.97\n",
      "subtree (51) = 0.93\n",
      "subtree (52) = 0.97\n",
      "subtree (53) = 0.93\n",
      "subtree (54) = 0.93\n",
      "subtree (55) = 0.80\n",
      "subtree (56) = 1.00\n",
      "subtree (57) = 0.97\n",
      "subtree (58) = 1.00\n",
      "subtree (59) = 0.90\n",
      "subtree (60) = 0.97\n",
      "subtree (61) = 1.00\n",
      "subtree (62) = 0.97\n",
      "subtree (63) = 0.93\n",
      "subtree (64) = 0.97\n",
      "subtree (65) = 0.97\n",
      "subtree (66) = 1.00\n",
      "subtree (67) = 1.00\n",
      "subtree (68) = 0.97\n",
      "subtree (69) = 0.93\n",
      "subtree (70) = 0.97\n",
      "subtree (71) = 1.00\n",
      "subtree (72) = 0.90\n",
      "subtree (73) = 0.93\n",
      "subtree (74) = 0.93\n",
      "subtree (75) = 1.00\n",
      "subtree (76) = 0.97\n",
      "subtree (77) = 0.97\n",
      "subtree (78) = 0.93\n",
      "subtree (79) = 0.93\n",
      "subtree (80) = 0.93\n",
      "subtree (81) = 1.00\n",
      "subtree (82) = 0.93\n",
      "subtree (83) = 0.93\n",
      "subtree (84) = 0.93\n",
      "subtree (85) = 0.90\n",
      "subtree (86) = 0.93\n",
      "subtree (87) = 0.97\n",
      "subtree (88) = 0.90\n",
      "subtree (89) = 0.97\n",
      "subtree (90) = 0.97\n",
      "subtree (91) = 0.93\n",
      "subtree (92) = 0.97\n",
      "subtree (93) = 0.97\n",
      "subtree (94) = 0.93\n",
      "subtree (95) = 1.00\n",
      "subtree (96) = 0.87\n",
      "subtree (97) = 1.00\n",
      "subtree (98) = 0.97\n",
      "subtree (99) = 0.97\n",
      "\n",
      "class-0 precision : 1.00\n",
      "class-1 precision : 0.71\n",
      "class-2 precision : 1.00\n",
      "\n",
      "class-0 recall : 1.00\n",
      "class-1 recall : 1.00\n",
      "class-2 recall : 0.86\n",
      "\n",
      "class-0 f1-score : 1.00\n",
      "class-1 f1-score : 0.83\n",
      "class-2 f1-score : 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# iris 데이터를 읽어온다.\n",
    "iris = load_iris()\n",
    "\n",
    "# Train 데이터 세트와 Test 데이터 세트를 구성한다\n",
    "trainX, testX, trainY, testY = \\\n",
    "    train_test_split(iris['data'], iris['target'], test_size = 0.2)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "rf.fit(trainX, trainY)\n",
    "\n",
    "# 학습데이터와 시험데이터의 정확도를 측정한다.\n",
    "print('\\n학습데이터의 정확도 = %.2f' % rf.score(trainX, trainY))\n",
    "print('시험데이터의 정확도 = %.2f' % rf.score(testX, testY))\n",
    "\n",
    "# 시험데이터의 confusion matrix를 작성한다 (row : actual, col : predict)\n",
    "predY = rf.predict(testX)\n",
    "print('\\nConfusion matrix :')\n",
    "print(confusion_matrix(testY, predY))\n",
    "print()\n",
    "print(classification_report(testY, predY, target_names=iris.target_names))\n",
    "\n",
    "# Sub tree별 시험데이터의 정확도를 확인한다.\n",
    "print('\\nSubtree별 시험데이터 정확도 :')\n",
    "for i in range(100):\n",
    "    subTree = rf.estimators_[i]\n",
    "    print('subtree (%d) = %.2f' % (i, subTree.score(testX, testY)))\n",
    "\n",
    "# classification_report()를 해석해 본다.\n",
    "import numpy as np\n",
    "label = np.vstack([testY, predY]).T\n",
    "\n",
    "# class = n이라고 예측한 것 중 실제 classe=n인 비율\n",
    "def precision(n):\n",
    "    y = label[label[:, 1] == n]\n",
    "    match = y[y[:, 0] == y[:, 1]]\n",
    "    return match.shape[0] / y.shape[0]\n",
    "\n",
    "print()\n",
    "print('class-0 precision : %.2f' % precision(0))\n",
    "print('class-1 precision : %.2f' % precision(1))\n",
    "print('class-2 precision : %.2f' % precision(2))\n",
    "\n",
    "# 실제 class = n인 것중 classe=n으로 예측한 비율\n",
    "def recall(n):\n",
    "    y = label[label[:, 0] == n]\n",
    "    match = y[y[:, 0] == y[:, 1]]\n",
    "    return match.shape[0] / y.shape[0]\n",
    "\n",
    "print()\n",
    "print('class-0 recall : %.2f' % recall(0))\n",
    "print('class-1 recall : %.2f' % recall(1))\n",
    "print('class-2 recall : %.2f' % recall(2))\n",
    "\n",
    "# F1-score\n",
    "def f1_score(n):\n",
    "    p = precision(n)\n",
    "    r = recall(n)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "print()\n",
    "print('class-0 f1-score : %.2f' % f1_score(0))\n",
    "print('class-1 f1-score : %.2f' % f1_score(1))\n",
    "print('class-2 f1-score : %.2f' % f1_score(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![800px-Lower_Manhattan_skyline_-_June_2017](markdown-images/800px-Lower_Manhattan_skyline_-_June_2017.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
