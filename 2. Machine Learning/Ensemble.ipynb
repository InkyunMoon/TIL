{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블\n",
    "- 단일 알고리즘을 사용하는 것보다 여러 알고리즘을 사용한 후 결과를 종합하는 것이 더 좋다.\n",
    "    - 정확도와 일반화 특성 증가\n",
    "    \n",
    "- Voting classifier  \n",
    "- Bagging  \n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "> 단일 머신러닝 모델들을 활용하여 예측결과를 조합(평균)하는 가장 간단한 앙상블 기법\n",
    "- 전혀 다른 모형들도 결합할 수 있다.\n",
    "\n",
    "- Hard voting : 단순 투표. 개별 모형의 결과 기준\n",
    "- Soft voting : 가중치 투표. 개별 모형의 조건무 확률의 합 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n",
    "                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n",
    "                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n",
    "                                              ('LR',LogisticRegression(C=0.05)),\n",
    "                                              ('DT',DecisionTreeClassifier(random_state=0)),\n",
    "                                              ('NB',GaussianNB()),\n",
    "                                              ('svm',svm.SVC(kernel='linear',probability=True))\n",
    "                                             ], \n",
    "                       voting='soft').fit(train_X,train_Y)\n",
    "print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\n",
    "\n",
    "cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\n",
    "\n",
    "print('The cross validated score is',cross.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "> 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다. 트레이닝 데이터를 랜덤하게 선택해서 다수결 모형을 적용한다.\n",
    "\n",
    "- 분산이 큰 모델에서 좋은 성능을 낸다. KNN과 랜덤 포레스트에 대해서 사용해보도록 하자.\n",
    "\n",
    "sklearn의 ensemble패키지에 배깅 모형을 위한 BaggingClassifier클래스를 제공한다.\n",
    "\n",
    "- base_estimator: 기본 모형\n",
    "- n_estimators: 모형 개수. 디폴트 10\n",
    "- bootstrap:데이터 중복 사용 여부. 디폴트 True\n",
    "- max_samples: 데이터 샘플 중 선택할 샘플의 수 혹은 비율. 디폴트 1.0\n",
    "- bootstrap_features: 특징 차원의 중복 사용 여부. 디폴트 False\n",
    "- max_features: 다차원 독립 변수 중 선택할 차원의 수 혹은 비율 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "> 분류가 어려운 패턴에 더욱 집중하여 정확도를 높이는 방법\n",
    "- 분류기의 연속적 학습을 사용한다. -> 약한 모델의 점진적 향상\n",
    "\n",
    "- 서브 훈련 데이터 샘플을 만든다.\n",
    "- 첫 샘플링은 모든 데이터에 동일 가중치를 두어 샘플링한다.\n",
    "- 처음 샘플링된 데이터로 학습한다.\n",
    "- 분류가 잘못된 데이터의 가중치를 높이고 다시 샘플링한다.\n",
    "    - 잘못 분류된 패턴은 선택될 확률이 높아진다.\n",
    "- 새로운 샘플링으로 다시 학습한다.\n",
    "- 이를 반복하여 점차 분류하기 어려운 패턴들을 많이 선택하여 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost (Adaptive Boosting)\n",
    "- AdaBoost는 Decision Trees(DT)와 합쳐서 설명하는 것이 일반적이라고 한다.\n",
    "\n",
    "- AdaBoost는 하나의 노드에 두개의 잎으로 구성되어있다. 하나의 노드와 두개의 잎으로 구성된 트리를 스텀프라고 한다.\n",
    "\n",
    "- 일반적인 DT는 데이터의 모든 컬럼을 활용하여 의사결정을 할 수 있는데(컬럼마다 노드를 이용하여 구분한다), 스텀프는 하나의 컬럼밖에 사용할 수 없으므로 **weak learner(약한 분류기)**라고 불린다.\n",
    "\n",
    "- 일반적으로 DT는 각각의 트리가 분류에 동등한 투표권을 행사하지만, Adaboost에서는 마지막 스텀프가 다른 분류보다 더욱 큰 의사권을 행사한다.\n",
    "\n",
    "- DT에서는 트리의 생성 순서는 상관없다. 그러나 AB에서는 스텀프의 생성 순서가 다음에 영향을 미친다.\n",
    "\n",
    "AB 정리) 1. AB는 약한 학습기로 구성되어있다.\n",
    "2. 모든 스텀프는 동등하지 않다.\n",
    "3. 각 스텀프는 이전 스텀프의 오류를 고려한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AB 구성 순서)\n",
    "1. 분류하기 위해서 각 샘플의 가중치를 준다.(처음엔 랜덤하게)\n",
    "    1/샘플의 수\n",
    "\n",
    "2. 각 열마다 스텀프를 구성해서 지니인덱스가 가장 적은 것을 처음 스텀프로 결정한다.\n",
    "- 선택한 스텀프가 마지막 분류에서 얼마나 영향을 미치는지 계산한다.\n",
    "    - 잘못 분류된 샘플의 가중치의 합으로 에러를 구한다.\n",
    "    - 모든 샘플 에러의 합은 1이다. 따라서 에러는 0~1사이의 값을 가진다.\n",
    "    - 1/2log(1-total error/total error)로 amount of say를 구한다.\n",
    "\n",
    "3. 스텀프 만들 때 모든 샘플은 동등한 가중치를 가졌는데, 잘못 분류된 샘플에 대해서 더욱 가중치를 두고 다른 샘플 웨이트는 줄인다.\n",
    "\n",
    "잘못 분류한 샘플 가중치 = 기존 가중치 * exp(amount of say)\n",
    "exp의 그래프를 그려보면,\n",
    "AoS가 큰 경우,(잘 분류한 경우) 가중치를 크게 곱해주어 가중치 크게 증가\n",
    "AoS가 작은 경우(못 분류 한 경우) 가중치를 작게 곱해주어 가중치 작게 증가\n",
    "\n",
    "제대로 분류한 샘플 가중치 = 기존 가중치 * -exp(amount of say)\n",
    "AoS가 큰 경우, 0에 가까운 수 곱해주어 가중치 작게 만듬.\n",
    "AoS가 작은 경우, 1에 가까운 수 곱해주어 가중치 덜 작게 만듬.\n",
    "\n",
    "이렇게 구한 가중치들의 합이 1이 되도록 표준화한다.\n",
    "이론상으로 weighted gini index를 사용하여 분류할 수 있지만, 대신 새로운 샘플의 컬렉션을 만들기로 하자.\n",
    "0~1의 랜덤 숫자를 선택하는데, 샘플 웨이트를 분포처럼 사용해서 어느 지점에 떨어지는지 확인하고, 해당하는 샘플을 새 컬렉션에 담는다.\n",
    "이렇게 원본과 같은 수의 컬렉션이 구성될 때 까지 반복한 뒤, 전체적인 AB의 과정을 반복한다.\n",
    "-> 웨이트가 큰 구간의 샘플이 중복되어 여러번 선택될 것이다.\n",
    "\n",
    "이렇게 원하는 스텀프의 개수를 만들때 까지 혹은 최적의 핏이 될 때 까지 스텀프를 만든다.\n",
    "\n",
    "분류하는 방법)\n",
    "0과 1의 각 스텀프마다 amount of say를 구해서 그 합이 큰 쪽으로 분류한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "Gradient Boost for regression - josh starmer\n",
    "-> 연속형 변수 예측과 분류에 사용, 범용적이다. 따라서 알고리즘이 복잡하다. 그러나 대부분의 경우 연속형 변수 예측에 사용된다.\n",
    "\n",
    "part 1\n",
    "\n",
    "Adaboost와는 달리, GB는 하나의 잎을 만들음으로서 시작한다. 이 잎은 초기 예측을 의미한다.\n",
    "첫 추측은 평균\n",
    "-> 나무를 만든다.\n",
    "\n",
    "이 나무는 스텀프보다는 일반적으로 크지만, 여전히 나무의 크기를 제한한다. (이전의 트리에 기반하였지만) 이 예에서는 4개의 잎까지 진행할 것이지만, 일반적으로 8~32개의 잎으로 구성된다.\n",
    "\n",
    "그러므로, 에이다부스트처럼 이전 트리의 에러를 기반으로 나무들을 만들지만, 그 길이가 에이다부스트보다는 크다.\n",
    "\n",
    "이렇게 지어진 나무의 크기를 조정한다.\n",
    "\n",
    "그 다음 이전 나무의 에러를 기반으로 다른 나무를 만들고, 크기를 조정한다.(스케일링)\n",
    "\n",
    "절차)\n",
    "몸무게의 평균을 구한다.\n",
    "첫 트리의 에러를 기반으로 트리를 만든다.\n",
    "그 에러는 실제값과 예측값의 차이이다.\n",
    "88-71.2 = 16.8 <- 이것은 Pseudo Residual이라 부르는데 이것은 선형회귀에서 기반한 것이지만, 선형회귀가 아닌 그래디언트 부스트 작업을 하고 있다는 것을 리마인드 하기 위함이다.\n",
    "이제, 피쳐들을 가지고 잔차를 예측하고자 한다. - 몸무게가 아닌 잔차 예측\n",
    "설정한 리프의 개수까지만 리프를 만든다는 것에 유의하면서 하나의 트리에 대해 분류한다.\n",
    "현재 한 리프에 있는 두개이상의 데이터들에 대해서 평균을 취해 하나로 만들어준다.\n",
    "이렇게 만들어진 트리와, 초기 트리를 이용해서 몸무게에 대한 추정을 한다.\n",
    "초기 트리 71.2 + learning rate(0~1) * 잔차\n",
    "-> 71.2+0.1*16.8 =72.9 이것은 초기 예측보다 조금 나은 수치이지만, 러닝레이트가 없을 때 보다는 덜하다.\n",
    "-> 학습률이 없으면 처음의 실제값과 같은 수치가 나오는데 이것은 분산이 매우 큰 추정이 될 것이므로 학습률을 설정한다.\n",
    "→ 올바른 방향으로 조금씩 움직이는 것은 테스트 셋으로 예측한 것에 대해 더 나은 성능을 보여준다. 예를들어 분산이 더 낮다.\n",
    "\n",
    "이전처럼 다시 잔차를 구해주자.\n",
    "88 - 72.9 = 15.1…\n",
    "이 예에서는 각각의 가지(트리 내의)가 전과 같지만, 실제에서는 각각 다를 수 있다.\n",
    "위와 같이 러닝레이트를 포함하여 무게에 대한 추정 반복…트리를 더 만드는 것이 유의미하게 잔차를 줄이지 않을 때 까지 반복\n",
    "\n",
    "\n",
    "part3\n",
    "\n",
    "Gradient Boost for Classification\n",
    "\n",
    "파트1처럼, 모든 데이터에 대한 초기 예측을 가진 리프로 시작한다. GB를 classification으로 사용할 때, 초기 예측은 Log(odds)이다.\n",
    "\n",
    "현재 예에서 트롤2를 좋아하는 사람은 4명, 싫어하는 사람은 2명이므로 초기 예측값은\n",
    "Log(4/2) = 0.7이다. 이것을 어떻게 분류에 사용할까?\n",
    "-> 로지스틱 리그레션처럼, 확률로 변환하는 것.(로지스틱 펑션을 사용한다.)\n",
    "→ exp(log(odds))/1+exp(log(odds)) = 0.7(반올림해서 같음)\n",
    "\n",
    "이 값이 0.7로 0.5보다 크므로, 이 데이터셋의 모든 사람들은 트롤2를 좋아한다고 생각할 수 있다. 그러나 사실 2명이 그 영화를 싫어하는데, 이 초기 예측의 불완전함을 pseudo residuals를 계산함으로써 측정할 수 있다.\n",
    "\n",
    "(observed - predicted)를 모든 데이터에 대해서 계산한다. -> 잔차 계산\n",
    "1 - 0.7(확률로 변환한 로그오즈 값) = 0.3\n",
    "1 - 0.7 = 0.3 \n",
    "0 - 0.7 = -0.7…\n",
    "\n",
    "예측들로부터 잔차를 구했다. 이것을 GB for regression때와 같이 트리를 구성한다(여전히 리프의 개수를 제한한다)\n",
    "\n",
    "GB 리그레션시, 값이 하나밖에 없는 잎은 아웃풋 값이 잔차와 같았다..\n",
    "그러나 분류 문제에서는 조금 복잡해진다. 예측이 로그오즈비로부터 만들어졌기 때문에.\n",
    "<- 잎은 확률로부터 도출되었는데, 예측은 로그오즈로부터 도출되었으므로.\n",
    "따라서, 로그오즈비 예측을 위해서 변환을 해주어야한다.\n",
    "각각의 잎에 대해서 아래의 공식을 이용하여 확률값으로 변환해준다.\n",
    "\n",
    "$$\n",
    "\\frac{\\sum{Residual_i}}{{\\sum{[Previous Probability_i * (1-Previous Probability_i)]}}}\n",
    "$$\n",
    "\n",
    "이렇게해서 변환해준 값을, 초기 로그오즈 예측값과, 학습률을 이용하여 다시 예측해준다.\n",
    "개인에 대한 예측값(로그오즈)을 구하고, 이것을 확률로 변환해주면 그 개인에 대한 트롤2를 좋아할 확률이 구해진다.\n",
    "\n",
    "\n",
    "이것을 각각의 데이터에 대해서 반복한다.\n",
    "즉, 초기 예측(로그오즈)값 -> 확률변환하여 잔차 구하기 -> 트리를 이용하여 잔차 분류 -> 분류된 잔차들을 로그오즈로 변환(싱글밸류 리프는 놔두고 여러개는 공식을 통해 로그오즈로 변환) -> 초기 예측(로그오즈)값과 학습률, 그리고 새로운 로그오즈 값을 이용하여 새로운 (로그오즈)예측을 한다. -> 이 값을 시그모이드 함수로 확률로 변환, 잔차구하기...반복(잔차가 굉장히 작아질때까지 or 지정한 트리 개수가 될 때 까지)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 사이트\n",
    "[데이터 사이언스 스쿨](https://datascienceschool.net/view-notebook/766fe73c5c46424ca65329a9557d0918/)  \n",
    "[Josh Starmer 유튜브 채널](https://www.youtube.com/user/joshstarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
