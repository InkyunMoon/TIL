{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                     'ml/machine-learning-databases/'\n",
    "                     'wine/wine.data',header=None)\n",
    "\n",
    "X, y =df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) 특성 스케일 맞추기\n",
    "- 의사결정나무와 랜덤 포레스트는 스케일 조정에 걱정할 필요가 없지만, 대부분 머신러닝과 최적화 알고리즘은 특성의 스케일이 같을 때 성능이 좋다.\n",
    "- 대표적 스케일링 방법\n",
    "    - 1. Normalization(정규화)\n",
    "    - 2. Standardization(표준화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalization\n",
    "- 대부분의 정규화는 특성의 스케일을 [0, 1] 범위에 맞추는 것.\n",
    "- 범위가 정해진 값이 필요할 때 유용하게 사용할 수 있다.\n",
    "$$\n",
    "x_{norm}^{(i)} = \\frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}  \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "x_{norm}^{(i)}: 특정샘플\n",
    "$$\n",
    "$$\n",
    "x_{min}: 특성 중에서 가장 작은 값\n",
    "$$\n",
    "$$x_{max}: 가장 큰 값$$\n",
    "\n",
    "- fit 메서드는 학습용 데이터에만 적용해야 한다. 학습 데이터의 통계치로 작업을 수행하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "x_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standardization\n",
    "- 특성의 평균을 0에 맞추고 표준편차를 1로 만들어 정규 분포와 같은 특징을 가지도록 만든다.\n",
    "- 이는 가중치를 더 쉽게 학습할 수 있도록 한다.\n",
    "- 이상치 정보가 유지되기 때문에 min-max scaler보다 이상치에 덜 민감하다.\n",
    "$$\n",
    "x_{std}^{(i)} = \\frac{x^{(i)}-\\bar{X}}{S_{x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{X}: 표본 평균\n",
    "$$\n",
    "$$\n",
    "S_{x}: 표준 편차\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표준화: [-1.46385011 -0.87831007 -0.29277002  0.29277002  0.87831007  1.46385011]\n",
      "정규화: [0.  0.2 0.4 0.6 0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "ex = np.array([0,1,2,3,4,5])\n",
    "print('표준화:', (ex-ex.mean())/ex.std())\n",
    "\n",
    "print('정규화:', (ex-ex.min())/(ex.max()-ex.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
