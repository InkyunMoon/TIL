{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree & Random Forest\n",
    "\n",
    "이 글은 인사이트 캠퍼스의 조성현 강사님의 강의를 수강하며 정리한 자료입니다. [조성현 강사님 블로그 바로가기](https://blog.naver.com/chunjein)\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "![image-20200718234831922](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- 직선 혹은 (초)평면으로 데이터를 분류하는 분류 알고리즘\n",
    "\n",
    "- 경계를 많이 사용하면 과잉적합의 우려가 있다. 따라서 정지기준, 사전/사후 가지치기 방법이 사용된다.\n",
    "  - 정지기준: 트리의 깊이 지정, 마지막 노드의 데이터 수 일정 수준이하로 떨어지지 않도록 지정\n",
    "  - 가지치기: 트리를 단순화시킨다.\n",
    "    - 사전 가지치기: 조기 정지규칙에 의해 트리 성장을 멈춤\n",
    "    - 사후 가지치기: 초기에 트리를 최대크기로 만들고, Trimming을 통해 복잡도를 줄여나간다.\n",
    "\n",
    "- 분류나 예측의 근거를 알 수 있으므로 쉽게 이해 가능\n",
    "\n",
    "- Feature마다 분류에 영향을 미치는 정도를 파악할 수 있다.\n",
    "\n",
    "  \n",
    "\n",
    "### 의사결정선 결정하기\n",
    "\n",
    "- 분할 전 부모 노드의 불순척도보다 분할 후 자식 노드의 불순척도가 낮을수록 좋다. -> 여러 분할을 시도하고 불순도가 가장 낮은 경계로 분할한다.\n",
    "\n",
    "- 불순척도\n",
    "\n",
    "  - 엔트로피(0~1의 값)\n",
    "    $$\n",
    "    H(t) = -\\sum\\limits_{i=1}^c{p(i|t)log_2p(i|t)}\n",
    "    $$\n",
    "\n",
    "  - 지니 지수(0~0.5의 값)\n",
    "    $$\n",
    "    Gini(t) = 1-\\sum\\limits_{i=1}^cp(i|t)^2\n",
    "    $$\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Pre-pruning(사전 가지치기) - depth로 최적화\n",
    "\n",
    "- 트리의 깊이를 변화시켜가면서 test데이터의 정확도를 측정, 가장 높은 깊이를 찾아 최종 Decision Tree로 사용\n",
    "\n",
    "```python\n",
    "for k in range(1,20):\n",
    "    dt = DecisionTreeClassifier(criterion = 'gini', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainGini.append((dt.predict(trainX)==trainY).mean())\n",
    "    testGini.append((dt.predict(testX)==testY).mean())\n",
    "    \n",
    "    dt = DecisionTreeClassifier(criterion ='entropy', max_depth = k)\n",
    "    dt.fit(trainX, trainY)\n",
    "    \n",
    "    trainEntropy.append(dt.score(trainX,trainY))\n",
    "    testEntropy.append(dt.score(testX,testY))\n",
    "```\n",
    "\n",
    "![image-20200719074811654](markdown-images/image-20200719074811654.png)\n",
    "\n",
    "- test dataset으로 측정한 정확도가 낮아지는 k=7 근처로 트리의 깊이를 지정한다.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Post-pruning(사후 가지치기) - α로 최적화\n",
    "\n",
    "- 최대한 복잡하게 트리를 구성한 뒤, 에러가 작아지는 방향으로 가지치기를 수행.\n",
    "\n",
    "- $$\n",
    "  오분류율: e_a(T) = \\frac{e+N*α}D\n",
    "  $$\n",
    "\n",
    "  - **<u>오분류율에 패널티항(α)을 추가하고 leaf-node가 많아질 수록 패널티를 크게 부여한다.</u>**\n",
    "    - e: 잘못 분류된 개수(leaf-node의 숫자가 작은 것)\n",
    "    - N: leaf-node개수\n",
    "    - D: data 개수\n",
    "\n",
    "- 즉, α=0부터 무한대까지 관찰하며 최적의 알파를 찾는다.\n",
    "\n",
    "```python\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(trainX, trainY)\n",
    "#이 함수는 학습데이터에 대해서 내부적으로 알파값을 변화시켜가면서 트리를 만들고 에러를 측정한다.\n",
    "#path에 impurities(불순도)와 alpha(패널티)가 리턴된다.\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "path\n",
    ">>>\n",
    "{'ccp_alphas': array([ 0.00000000e+00, -2.71050543e-20,  0.00000000e+00, ...,\n",
    "         1.56445662e-02,  2.65423192e-02,  6.05744854e-02]),\n",
    " 'impurities': array([0.03445308, 0.03445308, 0.03445308, ..., 0.28040352, 0.30694584,\n",
    "        0.36752033])}\n",
    "\n",
    "clfs = []\n",
    "for i, ccp_alpha in enumerate(ccp_alphas):\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(trainX, trainY)\n",
    "    clfs.append(clf)\n",
    "        \n",
    "    print('%d) ccp_alphas = %.4f done.' % (i, ccp_alpha))\n",
    "\n",
    "print('마지막 tree의 노드 개수 = %d' % clfs[-1].tree_.node_count)\n",
    "print('마지막 tree의 alpha = %.4f' % ccp_alphas[-1])\n",
    "print('마지막 tree의 depth = %d' % clfs[-1].tree_.max_depth)\n",
    "\n",
    ">>>\n",
    "마지막 tree의 노드 개수 = 3\n",
    "마지막 tree의 alpha = 0.0265\n",
    "마지막 tree의 depth = 1\n",
    "\n",
    "# clfs는 앞 부분 n개만 사용한다. 뒷 부분은 alpha가 너무 크기 때문에 제외한다.\n",
    "# test_scores[:n]중 가장 큰 최적 alpha를 찾는다.\n",
    "opt_alpha = ccp_alphas[np.argmax(test_scores[:n])]\n",
    "\n",
    "# opt_alpha를 적용한 tree를 사용한다.\n",
    "dt = DecisionTreeClassifier(ccp_alpha=opt_alpha)\n",
    "dt.fit(trainX, trainY)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Feature importrance\n",
    "\n",
    "- Decision Tree는 하위 tree의 양쪽 불순도가 최소가 되도록 분기를 결정한다. 분기를 완료한 뒤, feature별로 얼마나 불순도를 감소시켰는지 평균감소율을 계산해 feature들의 중요도를 판단할 수 있다.\n",
    "\n",
    "```python\n",
    "feature_importance = dt.feature_importances_\n",
    "feature_name = list(income.columns)\n",
    "n_feature = trainX.shape[1]\n",
    "idx = np.arange(n_feature)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
