{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[딥 러닝을 이용한 자연어 처리 입문(바로가기)](https://wikidocs.net/30707)을 정리한 내용입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링\n",
    "- **문서 집합의 추상적인 주제를 발견**하기 위한 모델링\n",
    "- 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA개요\n",
    "- TF-IDF와 같은 빈도수 기반 표현방법은 단어의 의미를 고려하지 못한다.\n",
    "- 따라서, 잠재된(Latent)의미를 이끌어내는 방법으로 LSA가 사용되었다.(SVD에 대한 이해 필요)\n",
    "- 즉, DTM이나 TF-IDF행렬에 truncated SVD를 사용해서 차원을 축소, 잠재적 의미를 이끌어낸다는 아이디어\n",
    "\n",
    "\n",
    "## 1) 일반적 SVD(Full SVD)\n",
    "- m * n 행렬 A를 다음과 같이 분해하는 것.\n",
    "$$A = UΣV^T$$\n",
    "\n",
    "$$U: m x m 직교행렬(AA^T = U(ΣΣ^T)U^T)$$\n",
    "$$V: n x n 직교행렬(A^TA = V(Σ^TΣ)V^T)$$\n",
    "$$Σ: m x n 직사각  대각행렬$$\n",
    "- 행렬Σ의 주대각원소를 행렬 A의 특이값이라고 하며, 내림차순으로 정렬되어있다.\n",
    "\n",
    "## 2) Truncated SVD\n",
    "- LSA의 경우 SVD에서 나온 행렬들을 일부 삭제하여 사용한다.\n",
    "     - 설명력이 낮은 정보 삭제 -> 계산비용 낮춤\n",
    "- Σ의 대각원소의 값 중 상위 t개의 값만 남긴다.\n",
    "    - t는 찾고자 하는 토픽의 수\n",
    "    - 크게잡으면 다양한 의미 가져갈 수 있지만 노이즈 존재\n",
    "- 절단시키면 값의 손실이 일어나 A행렬로 복원할 수 없다.\n",
    "\n",
    "\n",
    "Σ에서 t개의 topics만 선택했다고 해보자.  \n",
    "> U: m x t -> 문서 개수 x 토픽의 수  \n",
    "VT: t x n -> 토픽의 수 x 단어 개수의 크기  \n",
    "Σ : t x t  \n",
    "\n",
    "각 행렬들이 위와 같은 사이즈로 결정될 것이다.\n",
    "\n",
    "- U의 각 행은 잠재 의미를 표현하기 위한 수치화된 각각의 문서 벡터  \n",
    "\n",
    "- VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA 실습\n",
    "sklearn의 20newsgroups 데이터셋 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "with open('c:/data/news.data', 'rb') as f:\n",
    "    dataset  = pickle.load(f)\n",
    "\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리\n",
    "- 알파벳 제외한 구두점, 숫자, 특수문제 제거\n",
    "- 길이 짧은 단어 제거\n",
    "- 모든 알파벳 소문자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# str로 바꿔주지 않으면 \\n과 같은 이스케이프 코드를 삭제하지 않는다.\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# lambda 함수부분: 한 행씩 x로 받아서 공백을 기준으로 나눈 리스트를 만들고, 그 리스트의 요소(단어 하나씩)마다 길이가 3이면 문자열로 합친뒤, 그 값으로 초기화한다.\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist will speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument with murphy scared hell when came las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased what disagre...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well will have change scoring playoff pool unf...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist will speak...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  argument with murphy scared hell when came las...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].map(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.map(lambda x: [item for item in x if item not in stop_words])\n",
    "# tokenized_doc = map(lambda x : [x를 이용한 리스트 만들기]) -> x로 리스트를 만들어 리턴함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/21 밤\n",
    "[여기](https://wikidocs.net/24949) 실습 전처리 진행하다가 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['clean_doc2'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n"
     ]
    }
   ],
   "source": [
    "for i in news_df['clean_doc'][0]:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased what disagree with your statement that media ruin israels reputation that rediculous media most israeli media world having lived europe realize that incidences such described letter have occured media whole seem ignore them subsidizing israels existance europeans least same degree think that might reason they report more clearly atrocities what shame that austria daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away after look jews treating other races when they power unfortunate'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([w for w in news_df['clean_doc'][0].split() if len(w)>3])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
