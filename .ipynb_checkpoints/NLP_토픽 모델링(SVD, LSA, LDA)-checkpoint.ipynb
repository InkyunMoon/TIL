{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[딥 러닝을 이용한 자연어 처리 입문(바로가기)](https://wikidocs.net/30707)을 정리한 내용입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 알아봐야 할 것\n",
    "- svd_model.components_ : SVD에서 VT행렬\n",
    "- .get_feature_names() : 카운트 기반 단어표현에서 vocabulary 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링\n",
    "- **문서 집합의 추상적인 주제를 발견**하기 위한 모델링\n",
    "- 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA개요\n",
    "- TF-IDF와 같은 빈도수 기반 표현방법은 단어의 의미를 고려하지 못한다.\n",
    "- 따라서, 잠재된(Latent)의미를 이끌어내는 방법으로 LSA가 사용되었다.(SVD에 대한 이해 필요)\n",
    "- 즉, DTM이나 TF-IDF행렬에 truncated SVD를 사용해서 차원을 축소, 잠재적 의미를 이끌어낸다는 아이디어\n",
    "\n",
    "\n",
    "## 1) 일반적 SVD(Full SVD)\n",
    "- m * n 행렬 A를 다음과 같이 분해하는 것.\n",
    "$$A = USV^T$$\n",
    "\n",
    "$$U: m x m 직교행렬(AA^T = U(SS^T)U^T)$$\n",
    "$$V: n x n 직교행렬(A^TA = V(S^TS)V^T)$$\n",
    "$$S: m x n 직사각  대각행렬$$\n",
    "- 행렬Σ의 주대각원소를 행렬 A의 특이값이라고 하며, 내림차순으로 정렬되어있다.\n",
    "\n",
    "## 2) Truncated SVD\n",
    "- LSA의 경우 SVD에서 나온 행렬들을 일부 삭제하여 사용한다.\n",
    "     - 설명력이 낮은 정보 삭제 -> 계산비용 낮춤\n",
    "- Σ의 대각원소의 값 중 상위 t개의 값만 남긴다.\n",
    "    - t는 찾고자 하는 토픽의 수\n",
    "    - 크게잡으면 다양한 의미 가져갈 수 있지만 노이즈 존재\n",
    "- 절단시키면 값의 손실이 일어나 A행렬로 복원할 수 없다.\n",
    "\n",
    "\n",
    "Σ에서 t개의 topics만 선택했다고 해보자.  \n",
    "> U: m x t -> 문서 개수 x 토픽의 수  \n",
    "VT: t x n -> 토픽의 수 x 단어 개수의 크기  \n",
    "Σ : t x t  \n",
    "\n",
    "각 행렬들이 위와 같은 사이즈로 결정될 것이다.\n",
    "\n",
    "- U의 각 행은 잠재 의미를 표현하기 위한 수치화된 각각의 문서 벡터  \n",
    "\n",
    "- VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA 실습\n",
    "sklearn의 20newsgroups 데이터셋 사용하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "with open('c:/data/news.data', 'rb') as f:\n",
    "    dataset  = pickle.load(f)\n",
    "\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리\n",
    "- 알파벳 제외한 구두점, 숫자, 특수문제 제거\n",
    "- 길이 짧은 단어 제거\n",
    "- 모든 알파벳 소문자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# str로 바꿔주지 않으면 \\n과 같은 이스케이프 코드를 삭제하지 않는다.\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# lambda 함수부분: 한 행씩 x로 받아서 공백을 기준으로 나눈 리스트를 만들고, 그 리스트의 요소(단어 하나씩)마다 길이가 3이면 문자열로 합친뒤, 그 값으로 초기화한다.\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist will speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument with murphy scared hell when came las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased what disagre...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well will have change scoring playoff pool unf...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist will speak...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  argument with murphy scared hell when came las...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].map(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.map(lambda x: [item for item in x if item not in stop_words])\n",
    "# tokenized_doc = map(lambda x : [x를 이용한 리스트 만들기]) -> x로 리스트를 만들어 리턴함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer는 토큰화되어있지 않은 텍스트를 입력으로 사용한다.\n",
    "# 따라서 TF-IDF 행렬 만들기 위해 역토큰화(Detokenization)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure story seem biased disagree statement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize principle strongest points wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss proposal much ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well change scoring playoff pool unfortunately...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist speaking t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet orange micros grappler system upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument murphy scared hell came last year han...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure story seem biased disagree statement...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize principle strongest points wo...  \n",
       "3      notwithstanding legitimate fuss proposal much ...  \n",
       "4      well change scoring playoff pool unfortunately...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist speaking t...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet orange micros grappler system upd...  \n",
       "11313  argument murphy scared hell came last year han...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features = 1000, \n",
    "                             max_df = 0.5,\n",
    "                            # vocabulary 만들때 document frequency가 설정한 값보다 높으면 무시한다.(디폴트 1.0)\n",
    "                             smooth_idf = True)\n",
    "                             # document frequencies에 1을 더한다. -> 0으로 나누는 것을 방지(디폴트 True)\n",
    "    \n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased, disagree, st...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4        [well, change, scoring, playoff, pool, unfortu...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein, israeli, journalist, speak...\n",
       "11310                                                   []\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy, scared, hell, came, last, y...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized',n_iter=100,random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)\n",
    "# svd_model.components_는 VT에 해당된다.\n",
    "# 정확하게 토픽의 수 t x 단어의 수의 크기를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf = TfidfVectorizer(max_features=1000).get_feature_names\n",
    "\n",
    "# get_feature_names()가 모든 vectorizer에서 사용되는지 알아보기위한 시실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "# .get_feature_names()로 단어집합 term 객체 생성.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "> 문서 집합으로부터 어떤 토픽이 존재하는지 알아내기 위한 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA의 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생선한다고 가정한다.\n",
    "\n",
    "데이터가 주어지면 LDA는 문서가 생성되던 과정을 역추적한다.\n",
    "\n",
    "- LDA를 수행할 때, 문서 집합에서 토픽이 몇 개가 존재할지 가정하는것은 분석자의 몫.\n",
    "- **각 문서의 토픽 분포와 각 토픽 내의 단어 분포를 추정**  \n",
    "(문서-토픽 & 토픽-단어)\n",
    "\n",
    "- DTM or TF-IDF 행렬을 입력으로 한다. 즉, **단어의 순서는 신경쓰지 않겠다**는 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 분석자가 토픽 개수(k) 지정\n",
    "\n",
    "2. 모든 단어를 k개 중 하나의 토픽에 할당\n",
    "    - 하나의 토픽을 랜덤으로 할당. -> 각 문서는 토픽을 가진다.(랜덤 할당이기에 틀린 상태)\n",
    "\n",
    "3. (반복) 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있다고 가정\n",
    "w를 아래 두 기준에 따라서 토픽을 재할당.\n",
    "    - p(topic t | document d): 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율\n",
    "    - p(word w | topic t): 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율\n",
    "    \n",
    "예) 두 문서가 있을 때, 문서1의 한 단어(A)의 토픽을 결정한다고 하자.\n",
    "- 문서1의 단어들이 어떤 토픽에 해당하는지 확인, A가 속할 토픽들의 분포 확인\n",
    "- A가 전체 문서에서 어떤 토픽에 할당되어져 있는지 확인.\n",
    "\n",
    "\n",
    "## LSA vs LDA\n",
    "LSA: DTM을 차원 축소하여 근접 단어들을 토픽으로 묶는다.  \n",
    "LDA: 단어가 특정 토픽에 존재할 확률과, 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출\n",
    "\n",
    "# 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased, disagree, st...\n",
       "1        [yeah, expect, people, read, actually, accept,...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4        [well, change, scoring, playoff, pool, unfortu...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein, israeli, journalist, speak...\n",
       "11310                                                   []\n",
       "11311    [agree, home, runs, clemens, always, memorable...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy, scared, hell, came, last, y...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSA실습에서 만든 tokenized_doc을 활용.\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어에 정수 인코딩 + 각 뉴스에서의 단어의 빈도수를 기록최종적으로 doc2bow를 이용하여 (word_id, word_frequency)의 형태로 바꾸고자 한다.\n",
    "\n",
    "먼저 corpora.Dictionary()를 사용하여 (인덱스: 해당 단어)의 딕셔너리를 구해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "#tornnized_doc(Series객체)의 각 단어들을 dictionary(인덱스:단어)를 활용하여 doc2bow(단어 인덱스:해당 단어 빈도)로 만들어준다.\n",
    "print(corpus[1]) # 두 번째 뉴스 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위에서 52의 인덱스를 가지는 단어는 well 이다.\n"
     ]
    }
   ],
   "source": [
    "# (52,1)은 인덱스번호 52를 가지는 단어가 두번째 뉴스에서 1번 등장했다는 의미이다.\n",
    "print('위에서 52의 인덱스를 가지는 단어는' ,dictionary[52],'이다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 모델 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TO_PICS = 20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics = NUM_TO_PICS,\n",
    "                                          id2word = dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "# passes : 알고리즘의 동작 횟수 - 알고리즘이 결정하는 토픽의 값이 적절히 수렴할 수 있도록 충분히 적당한 횟수를 정해주면 된다.\n",
    "# num_words : 출력하고자 하는 단어의 수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.013*\"would\" + 0.011*\"time\" + 0.010*\"like\" + 0.009*\"think\"')\n",
      "(1, '0.014*\"file\" + 0.010*\"available\" + 0.010*\"files\" + 0.010*\"window\"')\n",
      "(2, '0.016*\"pitt\" + 0.016*\"gordon\" + 0.015*\"pitching\" + 0.015*\"banks\"')\n",
      "(3, '0.016*\"picture\" + 0.011*\"sleeve\" + 0.010*\"rockefeller\" + 0.008*\"switzerland\"')\n",
      "(4, '0.016*\"would\" + 0.016*\"people\" + 0.007*\"think\" + 0.006*\"government\"')\n",
      "(5, '0.010*\"jesus\" + 0.007*\"believe\" + 0.007*\"would\" + 0.007*\"people\"')\n",
      "(6, '0.008*\"information\" + 0.007*\"public\" + 0.005*\"encryption\" + 0.005*\"national\"')\n",
      "(7, '0.009*\"like\" + 0.008*\"bike\" + 0.007*\"right\" + 0.007*\"much\"')\n",
      "(8, '0.011*\"eisa\" + 0.007*\"corp\" + 0.007*\"ieee\" + 0.006*\"astros\"')\n",
      "(9, '0.016*\"armenian\" + 0.013*\"turkish\" + 0.012*\"armenians\" + 0.008*\"russian\"')\n",
      "(10, '0.079*\"drive\" + 0.050*\"scsi\" + 0.042*\"disk\" + 0.030*\"hard\"')\n",
      "(11, '0.007*\"good\" + 0.006*\"also\" + 0.005*\"price\" + 0.005*\"mail\"')\n",
      "(12, '0.027*\"game\" + 0.024*\"team\" + 0.018*\"games\" + 0.017*\"play\"')\n",
      "(13, '0.024*\"char\" + 0.015*\"filename\" + 0.008*\"borland\" + 0.007*\"homosexuality\"')\n",
      "(14, '0.040*\"output\" + 0.037*\"entry\" + 0.032*\"file\" + 0.021*\"program\"')\n",
      "(15, '0.018*\"said\" + 0.014*\"went\" + 0.010*\"people\" + 0.009*\"came\"')\n",
      "(16, '0.064*\"israel\" + 0.058*\"jews\" + 0.039*\"israeli\" + 0.031*\"jewish\"')\n",
      "(17, '0.016*\"would\" + 0.016*\"thanks\" + 0.015*\"know\" + 0.013*\"anyone\"')\n",
      "(18, '0.014*\"wire\" + 0.011*\"ground\" + 0.009*\"condition\" + 0.009*\"wiring\"')\n",
      "(19, '0.054*\"space\" + 0.021*\"nasa\" + 0.012*\"launch\" + 0.010*\"earth\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 별 토픽 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(4, 0.4731768), (9, 0.14595816), (16, 0.36714587)]\n",
      "1 번째 문서의 topic 비율은 [(0, 0.34306315), (4, 0.17499757), (5, 0.28510228), (6, 0.12306397), (9, 0.027841998), (18, 0.02747191)]\n",
      "2 번째 문서의 topic 비율은 [(0, 0.107579134), (1, 0.060708635), (4, 0.66533196), (16, 0.15280618)]\n",
      "3 번째 문서의 topic 비율은 [(0, 0.21825007), (1, 0.024551325), (4, 0.34481), (6, 0.21188332), (10, 0.03379682), (11, 0.115184665), (17, 0.041183032)]\n",
      "4 번째 문서의 topic 비율은 [(0, 0.36672553), (3, 0.22471875), (12, 0.3392332), (14, 0.03968223)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(i, '번째 문서의 topic 비율은', topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보기 편하게 df 형태로 만들어보기\n",
    "(나중에 추가하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA sklearn 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0  [aba, decides, against, community, broadcastin...\n",
      "1  [act, fire, witnesses, must, be, aware, of, de...\n",
      "2  [a, g, calls, for, infrastructure, protection,...\n",
      "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
      "4  [air, nz, strike, to, affect, australian, trav...\n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0   [aba, decides, community, broadcasting, licence]\n",
      "1    [act, fire, witnesses, must, aware, defamation]\n",
      "2     [g, calls, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "print(text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text\n",
      "0       [aba, decide, community, broadcast, licence]\n",
      "1      [act, fire, witness, must, aware, defamation]\n",
      "2      [g, call, infrastructure, protection, summit]\n",
      "3          [air, nz, staff, aust, strike, pay, rise]\n",
      "4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos = 'v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 길이가 3이하인 단어에 제거\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word)>3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "text['headline_text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       decide community broadcast licence\n",
       "1       fire witness must aware defamation\n",
       "2    call infrastructure protection summit\n",
       "3                   staff aust strike rise\n",
       "4      strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['headline_text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 1000)\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00001783e-01 1.00001935e-01 1.00009183e-01 ... 1.00003242e-01\n",
      "  1.00002451e-01 1.00005588e-01]\n",
      " [1.00001784e-01 1.12967298e+03 1.00010368e-01 ... 1.00004030e-01\n",
      "  1.00004859e-01 7.42418314e+02]\n",
      " [1.00002549e-01 1.00001501e-01 3.48345083e+03 ... 1.00003412e-01\n",
      "  1.00001950e-01 1.00004703e-01]\n",
      " ...\n",
      " [1.00000863e-01 1.00001435e-01 1.00002551e-01 ... 1.00003941e-01\n",
      "  1.00001945e-01 1.00002981e-01]\n",
      " [1.00002460e-01 1.00002580e-01 1.00010173e-01 ... 1.00003458e-01\n",
      "  1.00001385e-01 1.00006822e-01]\n",
      " [1.00000306e-01 1.00001164e-01 1.00004623e-01 ... 1.00003728e-01\n",
      "  1.00001927e-01 1.00003424e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components=10,\n",
    "                                      learning_method='online',\n",
    "                                      random_state=777, max_iter=1)\n",
    "# learning_method : components_를 학습하기 위한 방법을 설정하는 옵션.\n",
    "# 데이터 사이즈가 많을 경우 online 방법이 빠르다.\n",
    "# batch : 각각의 EM 업데이트마다 모든 훈련데이터 사용\n",
    "# online : 각각의 EM 업데이트마다 mini-batch 방식 사용.\n",
    "lda_top = lda_model.fit_transform(X)\n",
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('queensland', 7581.82), ('change', 5732.86), ('2016', 5468.84), ('coast', 5351.31), ('year', 5350.75)]\n",
      "Topic 2: [('government', 8547.73), ('court', 7404.45), ('south', 6558.01), ('world', 6553.8), ('home', 5492.63)]\n",
      "Topic 3: [('kill', 5754.15), ('report', 5479.61), ('rural', 5470.91), ('school', 5340.98), ('jail', 4539.3)]\n",
      "Topic 4: [('attack', 6863.65), ('first', 5464.93), ('state', 4835.11), ('turnbull', 4171.82), ('time', 3739.29)]\n",
      "Topic 5: [('call', 7866.35), ('death', 5854.35), ('miss', 4381.62), ('women', 4166.18), ('accuse', 4007.25)]\n",
      "Topic 6: [('trump', 11711.23), ('charge', 8288.93), ('murder', 6205.86), ('take', 5746.65), ('warn', 5014.09)]\n",
      "Topic 7: [('australia', 13293.85), ('find', 8700.7), ('canberra', 5966.55), ('show', 5812.48), ('open', 5540.06)]\n",
      "Topic 8: [('police', 11841.04), ('australian', 10854.93), ('sydney', 8265.66), ('interview', 5949.19), ('brisbane', 4832.47)]\n",
      "Topic 9: [('melbourne', 7416.33), ('house', 5905.61), ('live', 5352.98), ('could', 5169.57), ('drug', 4179.89)]\n",
      "Topic 10: [('election', 7444.33), ('adelaide', 6643.33), ('perth', 6350.98), ('make', 5571.62), ('shoot', 4415.01)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print('Topic %d:' % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n -1:-1]])\n",
    "get_topics(lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
