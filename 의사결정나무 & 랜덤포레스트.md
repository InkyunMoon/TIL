# Decision Tree & Random Forest

이 글은 인사이트 캠퍼스의 조성현 강사님의 강의를 수강하며 정리한 자료입니다. [조성현 강사님 블로그 바로가기](https://blog.naver.com/chunjein)

## Decision Tree

![image-20200718234831922](C:%5CUsers%5Cmoon%5CDesktop%5CTIL%5Cmarkdown-images%5Cimage-20200718234831922.png)

- 직선 혹은 (초)평면으로 데이터를 분류하는 분류 알고리즘

- 경계를 많이 사용하면 과잉적합의 우려가 있다. 따라서 정지기준, 사전/사후 가지치기 방법이 사용된다.
  - 정지기준: 트리의 깊이 지정, 마지막 노드의 데이터 수 일정 수준이하로 떨어지지 않도록 지정
  - 가지치기: 트리를 단순화시킨다.
    - 사전 가지치기: 조기 정지규칙에 의해 트리 성장을 멈춤
    - 사후 가지치기: 초기에 트리를 최대크기로 만들고, Trimming을 통해 복잡도를 줄여나간다.

- 분류나 예측의 근거를 알 수 있으므로 쉽게 이해 가능

- Feature마다 분류에 영향을 미치는 정도를 파악할 수 있다.

  

### 의사결정선 결정하기

- 분할 전 부모 노드의 불순척도보다 분할 후 자식 노드의 불순척도가 낮을수록 좋다. -> 여러 분할을 시도하고 불순도가 가장 낮은 경계로 분할한다.

- 불순척도

  - 엔트로피(0~1의 값)
    $$
    H(t) = -\sum\limits_{i=1}^c{p(i|t)log_2p(i|t)}
    $$

  - 지니 지수(0~0.5의 값)
    $$
    Gini(t) = 1-\sum\limits_{i=1}^cp(i|t)^2
    $$
    

### 코드 설명

#### 1. Decision Tree (pre-pruning)

