-----
##### Discriminator & Generator 생성
```python
# Discriminator를 생성한다.
def BuildDiscriminator():
    x = Input(batch_shape = (None, nDInput))
    h = Dense(nDHidden, activation = 'relu')(x)
    Dx = Dense(nDOutput, activation = 'sigmoid')(h)
    model = Model(x, Dx)
    model.compile(loss = 'binary_crossentropy', optimizer = Adam(0.001))
    return model

# Generator를 생성한다.
def BuildGenerator():
    z = Input(batch_shape = (None, nGInput))
    h = Dense(nGHidden, activation = 'relu')(z)
    Gz = Dense(nGOutput, activation='linear')(h)
    return Model(z, Gz)

def getNoise(m, n=nGInput):
    z = np.random.uniform(-1., 1., size=[m, n])
    return z

# 두 분포 (P, Q)의 KL divergence를 계산한다.
def KL(P, Q):
    # 두 데이터의 분포를 계산한다
    histP, binsP = np.histogram(P, bins=100)
    histQ, binsQ = np.histogram(Q, bins=binsP)
    
    # 두 분포를 pdf로 만들기 위해 normalization한다.
    histP = histP / (np.sum(histP) + 1e-8)
    histQ = histQ / (np.sum(histQ) + 1e-8)
    
    # KL divergence를 계산한다
    kld = np.sum(histP * (np.log(histP + 1e-8) - np.log(histQ + 1e-8)))
    return kld

def getNoise(m, n=nGInput):
    z = np.random.uniform(-1., 1., size=[m, n])
    return z
```

##### GAN 생성 & 학습 준비

```python
def BuildGAN(D, G):
    D.trainable = False #1)
    z = Input(batch_shape=(None,nGIput))
    Gz = G(z) #2)
    DGz = D(Gz)
    
    model = Model(z, DGz)
    model.compile(loss = 'binary_crossentropy'), optimizer = Adam(0.0005)
    return model

K.clear_session() #3)

Discriminator = BuildDiscriminator()
Generator = BuildGenerator()
GAN = BuildGAN(Discriminator, Generator)

nBatchCnt = 3 # Mini-batch를 위해 input 데이터를 n개 블록으로 나눈다.
nBatchSize = int(realData.shape[0] / nBatchCnt)  # 블록 당 Size
```

(1) 앞서 compile 옵션을 설정했던 discriminator는 업데이트하지 않는다.

그동안 명시하지 않았어도 .trainable = True로 자동설정되어 모든 모델을 업데이트했는데, 이번의 경우에는 False로 설정하여 앞서 

(2) G라는 모델에 z를 입력으로 넣는다.

(3) ~~## 커널을 리스타트해도 그래프에 인풋 자료가 남아있다(?) 서머리를 해주는 경우 input_2. 리스타트 안하면 뒤의 숫자가 계속해서 증가하는데, 이것을 방지해준다.~~??

##### 모델 학습

```python
for epoch in range(1000):
    # Mini-batch 방식으로 학습한다.
    for n in range(nBatchCnt):
        # input 데이터를 Mini-batch 크기에 맞게 자른다.
        nFrom = n * nBatchSize
        nTo = n * nBatchSize + nBatchSize
        
        # 마지막 루프이면 nTo는 input 데이터의 끝까지.
        if n == nBatchCnt - 1:
            nTo = realData.shape[0]
                
        # 학습 데이터를 준비한다.
        bx = realData[nFrom : nTo]
        bz = getNoise(m=bx.shape[0], n=nGInput)
        Gz = Generator.predict(bz)

        # Discriminator를 학습한다.
        # Real data가 들어가면 Discriminator의 출력이 '1'이 나오도록 학습하고,
        # Fake data (Gz)가 들어가면 Discriminator의 출력이 '0'이 나오도록 학습한다.
        target = np.zeros(bx.shape[0] * 2)
        target[ : bx.shape[0]] = 0.9     # '1' 대신 0.9로 함
        target[bx.shape[0] : ] = 0.1     # '0' 대신 0.1로 함
        bx_Gz = np.concatenate([bx, Gz]) 
        ## bx(333,1)은 진짜 데이터이고, Gz(333,1)은 generator함수를 통해 가짜 데이터가 생성된 것.
		## 진짜 데이터는 0.9, 가짜 데이터는 0.1이 되도록 (진짜데이터: 가짜데이터)의 x, y값을 fit(train_on_batch)함수의 			  인자로 설정해준다.
        Dloss = Discriminator.train_on_batch(bx_Gz, target)
        
        # Generator를 학습한다.
        # Fake data (z --> Gz --> DGz)가 들어가도 Discriminator의 출력이 '1'이
        # 나오도록 Generator를 학습한다.
        target = np.zeros(bx.shape[0])
        target[:] = 0.9
        Gloss = GAN.train_on_batch(bz, target)
        
    if epoch % 10 == 0:
        z = getNoise(m=realData.shape[0], n=nGInput)
        fakeData = Generator.predict(z)
        kd = KL(realData, fakeData)
        print("epoch = %d, D-Loss = %.3f, G-Loss = %.3f, KL divergence = %.3f" % (epoch, Dloss, Gloss, kd))
```

