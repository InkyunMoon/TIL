{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PV-DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument # 문장마다 paragraph ID, tag ID\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다.',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "sentences = [s.split() for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['너', '오늘', '이뻐', '보인다.'],\n",
       " ['나는', '오늘', '기분이', '더러워'],\n",
       " ['나', '좋은', '일이', '생겼어'],\n",
       " ['아', '오늘', '진짜', '짜증나'],\n",
       " ['환상적인데,', '정말', '좋은거', '같아']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['너', '오늘', '이뻐', '보인다.'], tags=['d0']),\n",
       " TaggedDocument(words=['나는', '오늘', '기분이', '더러워'], tags=['d1']),\n",
       " TaggedDocument(words=['나', '좋은', '일이', '생겼어'], tags=['d2']),\n",
       " TaggedDocument(words=['아', '오늘', '진짜', '짜증나'], tags=['d3']),\n",
       " TaggedDocument(words=['환상적인데,', '정말', '좋은거', '같아'], tags=['d4'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [f'd{i}']) for i, doc in enumerate(sentences)]\n",
    "documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV-DM 모델을 생성한다.\n",
    "model = Doc2Vec(size = 5, alpha = 0.025, min_alpha = 0.00025, min_count = 1, dm = 1)\n",
    "# size: 벡터 크기(하나의 워드에 대해 5개의 벡터로 출력할 것\n",
    "# alpha: 학습율, min_count: 단어들이 최소 한번 쓰인 것을 가지고 모델을 만들어라\n",
    "# dm: 1, pd-dm으로 학습 0, pv-dbow로 학습\n",
    "# PV-DM 모델을 학습한다.\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples = len(samples), epochs = 100)\n",
    "# 태그 붙은 문장을 넣어주고, 전체 문장 개수(total_examples) 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09755615, -0.03389874, -0.09121843, -0.07583982, -0.05491541],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vector를 확인해본다.\n",
    "model.wv['보인다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04046106,  0.07178956, -0.07444929, -0.02096957,  0.01386234],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph vector를 확인해 본다.\n",
    "model.docvecs[0] # \"너 오늘 이뻐 보인다\"의 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04046106,  0.07178956, -0.07444929, -0.02096957,  0.01386234],\n",
       "       [-0.02511991,  0.0449729 ,  0.00684274, -0.09887459,  0.02967102],\n",
       "       [-0.0140421 , -0.09616086,  0.00980749,  0.05497954,  0.09260675],\n",
       "       [-0.00475911, -0.08969692, -0.08260957, -0.05395378, -0.09687751],\n",
       "       [ 0.01370306, -0.09600275,  0.09940554,  0.0561846 , -0.09013303]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc2vec을 쓰면 문장마다 하나의 벡터로 만들 수 있다.\n",
    "model.docvecs.vectors_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 문장에 대한 pv를 inference한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04696345, -0.10669488, -0.06712131, -0.03154682, -0.07034492],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference stage\n",
    "model.infer_vector('오늘 좋은 일이 있을 것 같아.'.split())\n",
    "# 5개의 벡터로 추론해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 살펴보기\n",
    "Doc2Vec과 Logistic Regression을 이용한 영화리뷰 데이터 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CLEAN_DATA = '4-1.train_clean.csv'\n",
    "DATA_IN_PATH = 'C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '4-1.300features.doc2vec'\n",
    "model_saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_saved:\n",
    "    model = Doc2Vec.load(DATA_IN_PATH + model_name)\n",
    "else:\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]\n",
    "\n",
    "    model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha = 0.00025,\n",
    "                   min_count=10, workers = 4, dm = 1)\n",
    "    model.build_vocab(documents)\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=10)\n",
    "    model.save(DATA_IN_PATH + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought']\n"
     ]
    }
   ],
   "source": [
    "keys = list(model.wv.vocab.keys())[:20]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5822584\n",
      "0.05612491\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('dog','cat'), model.wv.similarity('dog','cake'), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.470947\n",
      "2.869419\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(model.wv['dog'], model.wv['cat']), np.dot(model.wv['dog'], model.wv['cake']),sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chicken', 0.592168390750885),\n",
       " ('cat', 0.5822584629058838),\n",
       " ('puppy', 0.5540207624435425),\n",
       " ('cats', 0.5450679063796997),\n",
       " ('eat', 0.5350972414016724),\n",
       " ('dogs', 0.5316440463066101),\n",
       " ('bunny', 0.5241804122924805),\n",
       " ('worm', 0.5236088633537292),\n",
       " ('bite', 0.5121653079986572),\n",
       " ('bike', 0.5107408761978149)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = model.infer_vector(['system','response','cpu','compute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2\n",
    "X = np.array([model.docvecs[i] for i in range(len(sentences))])\n",
    "y = np.array(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT,\n",
    " random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = LogisticRegression(class_weight = 'balanced', solver = 'newton-cg')\n",
    "lgs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1]\n",
      "Accuracy: 0.845200\n"
     ]
    }
   ],
   "source": [
    "predicted = lgs.predict(X_eval)\n",
    "print(predicted[:20])\n",
    "print('Accuracy: %f' % lgs.score(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 48.1548 - accuracy: 0.7185\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4221 - accuracy: 0.8267\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.4002 - accuracy: 0.8363\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3984 - accuracy: 0.8383\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3954 - accuracy: 0.8371\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3944 - accuracy: 0.8377\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3876 - accuracy: 0.8421\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3878 - accuracy: 0.8424\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3887 - accuracy: 0.8410\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3907 - accuracy: 0.8407\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3869 - accuracy: 0.8422\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3872 - accuracy: 0.8432\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8449\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3887 - accuracy: 0.8415\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3939 - accuracy: 0.8381\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3899 - accuracy: 0.8414\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3874 - accuracy: 0.8431\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3841 - accuracy: 0.8421\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3876 - accuracy: 0.8433\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3926 - accuracy: 0.8392\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3844 - accuracy: 0.8439\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3806 - accuracy: 0.8446\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3791 - accuracy: 0.8465\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3867 - accuracy: 0.8450\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3783 - accuracy: 0.8479\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3794 - accuracy: 0.8468\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3789 - accuracy: 0.8472\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3800 - accuracy: 0.8463\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3863 - accuracy: 0.8443\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3766 - accuracy: 0.8465\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3764 - accuracy: 0.8476\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3836 - accuracy: 0.8458\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3746 - accuracy: 0.8483\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3779 - accuracy: 0.8496\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3803 - accuracy: 0.8447\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3755 - accuracy: 0.8479\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3845 - accuracy: 0.8475\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3773 - accuracy: 0.8476\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3762 - accuracy: 0.8488\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3783 - accuracy: 0.8475\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3842 - accuracy: 0.8469\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3769 - accuracy: 0.8471\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3755 - accuracy: 0.8485\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3784 - accuracy: 0.8479\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3804 - accuracy: 0.8475\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3768 - accuracy: 0.8482\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 39899.8320 - accuracy: 0.6982\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 579.9711 - accuracy: 0.7753\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 47.7523 - accuracy: 0.7743\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 41.3049 - accuracy: 0.7749\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.5729 - accuracy: 0.7789\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.8116 - accuracy: 0.7770\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 8.5569 - accuracy: 0.7749\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 14.0983 - accuracy: 0.7710\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 5.5161 - accuracy: 0.7721\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 5.0091 - accuracy: 0.7703\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.4428 - accuracy: 0.7720\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.5653 - accuracy: 0.7706\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.1103 - accuracy: 0.7732\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.0344 - accuracy: 0.7703\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.1020 - accuracy: 0.7746\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.7030 - accuracy: 0.7723\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.7189 - accuracy: 0.7705\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.2753 - accuracy: 0.7732\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.4143 - accuracy: 0.7675\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.6801 - accuracy: 0.7720\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.0193 - accuracy: 0.7727\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.6004 - accuracy: 0.7704\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.5410 - accuracy: 0.7780\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.4592 - accuracy: 0.7721\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.7762 - accuracy: 0.7699\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.6060 - accuracy: 0.7672\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.5678 - accuracy: 0.7699\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.3801 - accuracy: 0.7771\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.3887 - accuracy: 0.7735\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.0748 - accuracy: 0.7822\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.1231 - accuracy: 0.7745\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.9405 - accuracy: 0.7728\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.4658 - accuracy: 0.7739\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.0026 - accuracy: 0.7807\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.0529 - accuracy: 0.7775\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.9741 - accuracy: 0.7820\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 1558.6504 - accuracy: 0.7577\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 486.8333 - accuracy: 0.7689\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 16.2868 - accuracy: 0.7775\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.1560 - accuracy: 0.7821\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2.4306 - accuracy: 0.7812\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 1.4141 - accuracy: 0.7836\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 1.0669 - accuracy: 0.7893\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8270 - accuracy: 0.7865\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7101 - accuracy: 0.7972\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.8762 - accuracy: 0.7897\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8357 - accuracy: 0.7897\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.8036 - accuracy: 0.7940\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.7631 - accuracy: 0.7928\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6481 - accuracy: 0.7993\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5895 - accuracy: 0.8048\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.6210 - accuracy: 0.8036\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5561 - accuracy: 0.8041\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.5940 - accuracy: 0.8037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2022949af08>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xInput = Input(batch_shape = (None,300))\n",
    "xHidden1 = Dense(512)(xInput)\n",
    "xHidden2 = Dense(256)(xHidden1)\n",
    "xHidden3 = Dense(128)(xHidden2)\n",
    "xOutput = Dense(1, activation = 'sigmoid')(xHidden3)\n",
    "model2 = Model(xInput, xOutput)\n",
    "model2.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.05) ,metrics=['accuracy'])\n",
    "model2.fit(X_train,y_train, epochs=100, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8146"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred,y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec - 8/5 gensim 복습하고 돌아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x202296bf948>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 300 # 문자 벡터 차원 수\n",
    "min_word_count = 40 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 처리 스레드 수\n",
    "context = 10 # 문자열 창 크기\n",
    "downsampling = 1e-3 # 문자 빈도수 Downsample\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2VecKeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-5f01b611e162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2VecKeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "model.wv(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
