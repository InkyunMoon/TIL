{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "common_texts\n",
    "\n",
    "model = FastText(size = 5, window=3, min_count = 1, bucket=100)\n",
    "model.build_vocab(sentences = common_texts)\n",
    "model.train(sentences = common_texts, total_examples = len(common_texts), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03090127,  0.03628641,  0.0224218 , -0.01244347, -0.00975179],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['human']\n",
    "model.wv['klakasdfsdjf'] #  OOV인데 수치가 나온다. 신뢰하기 힘들지라도 어떤 숫자가 나오기는 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab\n",
    "model.wv.vocab.keys()\n",
    "\n",
    "model.wv.vectors_ngrams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bucket 개수만큼 shape가 잡힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4c24108812a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Quora 데이터의 Vocabulary를 읽어온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/5-2.vocabulary.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mword2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# maLSTM 모델을 빌드한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# Quora question pairs : maLSTM 텍스트 유사도 모델\n",
    "# \n",
    "# Pre-trained FastText를 적용한다.\n",
    "#\n",
    "# Download :\n",
    "# https://github.com/facebookresearch/fastText/blob/master/docs/pretrained-vectors.md\n",
    "# English: bin+text\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from gensim.models import fasttext\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# 전처리가 완료된 학습 데이터를 읽어온다.\n",
    "DATA_IN_PATH = 'C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/'\n",
    "TRAIN_Q1_DATA_FILE = '5-2.train_q1.npy'\n",
    "TRAIN_Q2_DATA_FILE = '5-2.train_q2.npy'\n",
    "TRAIN_LABEL_DATA_FILE = '5-2.train_label.npy'\n",
    "\n",
    "q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))\n",
    "\n",
    "# Quora 데이터의 Vocabulary를 읽어온다.\n",
    "with open('C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/5-2.vocabulary.pickle', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "    \n",
    "# maLSTM 모델을 빌드한다.\n",
    "VOCAB_SIZE = len(word2idx) + 1\n",
    "EMB_SIZE = 300         # pre-trained FastText의 vector size = 300이므로, 이와 동일하게 맞춘다.\n",
    "savedWeightEmbedding = True\n",
    "\n",
    "if savedWeightEmbedding:\n",
    "    # 저장된 WE를 읽어온다.\n",
    "    with open('C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/weightEmbedding(FastText).pickle', 'rb') as f:\n",
    "        WE = pickle.load(f)\n",
    "else:\n",
    "    # Pre-trained FastText 파일을 읽어와서 dictionary를 생성한다.\n",
    "    # 8GB 컴에서 MemoryError 발생. 16GB 이상 필요함.\n",
    "    model = fasttext.load_facebook_vectors('C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/wiki.en.bin')\n",
    "\n",
    "    WE = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
    "    \n",
    "    for word, i in word2idx.items():\n",
    "        WE[i] = model[word]\n",
    "    \n",
    "    # 결과를 저장한다.\n",
    "    with open('C:/inkyun/실습파일과 교재/5.자연어처리(실습파일)/dataset/weightEmbedding(FastText).pickle', 'wb') as f:\n",
    "        pickle.dump(WE, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 학습 데이터와 시험 데이터로 나눈다.\n",
    "trainQ1, testQ1, trainQ2, testQ2, trainY, testY = \\\n",
    "        train_test_split(q1_data, q2_data, labels, test_size=0.2)\n",
    "\n",
    "HIDDEN_DIM = 128\n",
    "FEATURE_DIM = 64\n",
    "\n",
    "# Question-1, 2 입력용\n",
    "K.clear_session()\n",
    "inputQ1 = Input(batch_shape=(None, trainQ1.shape[1]))\n",
    "inputQ2 = Input(batch_shape=(None, trainQ2.shape[1]))\n",
    "\n",
    "# 공통으로 사용할 Embedding layer\n",
    "embCommon = Embedding(input_dim=VOCAB_SIZE, output_dim=EMB_SIZE,\n",
    "                      weights = [WE], trainable = False)\n",
    "# Question-1 처리용 LSTM\n",
    "embQ1 = embCommon(inputQ1)\n",
    "embQ1 = Dropout(rate=0.5)(embQ1)\n",
    "lstmQ1 = LSTM(HIDDEN_DIM)(embQ1)\n",
    "lstmQ1 = Dense(FEATURE_DIM, activation='relu')(lstmQ1)\n",
    "lstmQ1 = Dropout(rate=0.5)(lstmQ1)\n",
    "\n",
    "# Question-2 처리용 LSTM\n",
    "embQ2 = embCommon(inputQ2)\n",
    "embQ2 = Dropout(rate=0.5)(embQ2)\n",
    "lstmQ2 = LSTM(HIDDEN_DIM)(embQ2)\n",
    "lstmQ2 = Dense(FEATURE_DIM, activation='relu')(lstmQ2)\n",
    "lstmQ2 = Dropout(rate=0.5)(lstmQ2)\n",
    "\n",
    "# Question-1, 2의 출력으로 맨하탄 거리를 측정한다.\n",
    "# lstmQ1 = lstmQ2 --> mDist = 1\n",
    "# lstmQ1 - lstmQ2 = inf --> mDist = 0\n",
    "# mDist = 0 ~ 1 사잇값이므로, trainY = [0, 1]과 mse를 측정할 수 있다.\n",
    "mDist = K.exp(-K.sum(K.abs(lstmQ1 - lstmQ2), axis=1, keepdims=True))\n",
    "\n",
    "model = Model([inputQ1, inputQ2], mDist)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0005))\n",
    "model.summary()\n",
    "\n",
    "# 학습\n",
    "trainY = trainY.reshape(-1, 1)\n",
    "testY = testY.reshape(-1, 1)\n",
    "hist = model.fit([trainQ1, trainQ2], trainY,\n",
    "                 validation_data = ([testQ1, testQ2], testY),\n",
    "                 batch_size = 1000, epochs = 10)\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# 시험 데이터로 학습 성능을 평가한다\n",
    "predicted = model.predict([testQ1, testQ2])\n",
    "predY = np.where(predicted > 0.5, 1, 0)\n",
    "accuracy = (testY == predY).mean()\n",
    "print(\"\\nAccuracy = %.2f %s\" % (accuracy * 100, '%'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
