{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "import collections\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import pickle\n",
    "\n",
    "# 전처리\n",
    "def MyPreprocessing(text):\n",
    "    text2 = \"\".join([\" \" if ch in string.punctuation else ch for ch in text])\n",
    "    tokens = nltk.word_tokenize(text2)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "\n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "# IMDB 데이터에 사용된 총 단어의 종류는 88,584개 (vocabulary 크기)이다.\n",
    "# 가장 많이 사용되는 6,000개 단어만 사용하고, 나머지는 OOV로 표시한다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=6000,\n",
    "                                                      start_char=0,\n",
    "                                                      oov_char=0,\n",
    "                                                      index_from=0)\n",
    "\n",
    "# train, test 데이터를 합친다. 필요한 경우 나중에 나눠쓴다.\n",
    "text = np.hstack([x_train, x_test])\n",
    "label = np.hstack([y_train, y_test])\n",
    "\n",
    "# vocabulary를 가져온다.\n",
    "word2idx = imdb.get_word_index()\n",
    "idx2word = dict((v,k) for k,v in word2idx.items())\n",
    "\n",
    "# start_char와 oov_char는 '.'으로 표시해 둔다. 나중에 전처리 과정에서 제거된다.\n",
    "idx2word[0] = '.'\n",
    "\n",
    "# 숫자로 표시된 x_train을 실제 단어로 변환한다.\n",
    "def decode(review):\n",
    "    x = [idx2word[s] for s in review]\n",
    "    return ' '.join(x)\n",
    "\n",
    "# 리뷰 문서를 전처리한다.\n",
    "reviews = []\n",
    "for i, review in enumerate(text):\n",
    "    reviews.append(MyPreprocessing(decode(review)))\n",
    "    if i % 100 == 0: print(i)\n",
    "\n",
    "# 전처리된 리뷰 문서로 vocabulary를 다시 생성한다.\n",
    "counter = collections.Counter()\n",
    "for line in reviews:\n",
    "    for word in nltk.word_tokenize(line):\n",
    "        counter[word.lower()] += 1\n",
    "\n",
    "word2idx2 = {w:(i+1) for i, (w, _) in enumerate(counter.most_common())}\n",
    "idx2word2 = {v:k for k,v in word2idx2.items()}\n",
    "\n",
    "# # 전처리된 리뷰 문서와 vocabulary를 저장한다.\n",
    "# with open('data/preprocessed_review.pickle', 'wb') as f:\n",
    "#     pickle.dump([reviews, label, word2idx2, idx2word2], f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'locat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-714763518368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# 사전에 부여된 번호로 단어들을 표시한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Trigram으로 주변 단어들을 묶는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-714763518368>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# 사전에 부여된 번호로 단어들을 표시한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Trigram으로 주변 단어들을 묶는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'locat'"
     ]
    }
   ],
   "source": [
    "# SkNS 학습. pre-trained embedding vector를 생성한다.\n",
    "# -------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.layers import Dot, Activation, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# # 전처리가 완료된 IMDB 데이터를 읽어온다.\n",
    "# with open('data/preprocessed_review.pickle', 'rb') as f:\n",
    "#     reviews, label, word2idx, idx2word = pickle.load(f)\n",
    "\n",
    "# Trigram으로 학습 데이터를 생성한다.\n",
    "xs = []     # 입력 데이터\n",
    "ys = []     # 출력 데이터\n",
    "for line in reviews:   \n",
    "    # 사전에 부여된 번호로 단어들을 표시한다.\n",
    "    embedding = [word2idx[w.lower()] for w in nltk.word_tokenize(line)]\n",
    "    \n",
    "    # Trigram으로 주변 단어들을 묶는다.\n",
    "    triples = list(nltk.trigrams(embedding))\n",
    "    \n",
    "    # 왼쪽 단어, 중간 단어, 오른쪽 단어로 분리한다.\n",
    "    w_lefts = [x[0] for x in triples]\n",
    "    w_centers = [x[1] for x in triples]\n",
    "    w_rights = [x[2] for x in triples]\n",
    "    \n",
    "    # 입력 (xs)      출력 (xy)\n",
    "    # ---------    -----------\n",
    "    # 중간 단어 --> 왼쪽 단어\n",
    "    # 중간 단어 --> 오른쪽 단어\n",
    "    xs.extend(w_centers)\n",
    "    ys.extend(w_lefts)\n",
    "    xs.extend(w_centers)\n",
    "    ys.extend(w_rights)\n",
    "\n",
    "# SGNS용 학습 데이터를 생성한다.\n",
    "rand_word = np.random.randint(1, len(word2idx), len(xs))\n",
    "x_pos = np.vstack([xs, ys]).T\n",
    "x_neg = np.vstack([xs, rand_word]).T\n",
    "\n",
    "y_pos = np.ones(x_pos.shape[0]).reshape(-1,1)\n",
    "y_neg = np.zeros(x_neg.shape[0]).reshape(-1,1)\n",
    "\n",
    "x_total = np.vstack([x_pos, x_neg])\n",
    "y_total = np.vstack([y_pos, y_neg])\n",
    "\n",
    "X = np.hstack([x_total, y_total])\n",
    "np.random.shuffle(X)\n",
    "\n",
    "# SGNS 모델을 생성한다.\n",
    "vocab_size = len(word2idx) + 1  # 사전의 크기\n",
    "EMB_SIZE = 64\n",
    "BATCH_SIZE = 10240\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "inputX = Input(batch_shape = (None, 1))\n",
    "inputY = Input(batch_shape = (None, 1))\n",
    "embX = Embedding(vocab_size, EMB_SIZE)(inputX)\n",
    "embY = Embedding(vocab_size, EMB_SIZE)(inputY)\n",
    "dotXY = Dot(axes=2)([embX, embY])\n",
    "dotXY = Reshape((1,))(dotXY)\n",
    "outputY = Activation('sigmoid')(dotXY)\n",
    "\n",
    "model = Model([inputX, inputY], outputY)\n",
    "model.compile(optimizer = \"adam\", loss=\"binary_crossentropy\")\n",
    "model.summary()\n",
    "\n",
    "# 학습\n",
    "hist = model.fit([X[:, 0], X[:, 1]], X[:, 2],\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=NUM_EPOCHS)\n",
    "\n",
    "# # Embedding (left side) layer의 W를 저장해 둔다\n",
    "# with open('data/embedding_W.pickle', 'wb') as f:\n",
    "#     pickle.dump(model.layers[2].get_weights(), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGNS로 pre-trained된 embedding vector를 이용해서\n",
    "# IMDB 리뷰 문서들을 분류한다 (감성분석).\n",
    "# -----------------------------------------------\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# # 전처리가 완료된 IMDB 데이터를 읽어온다.\n",
    "# with open('dataset/preprocessed_review.pickle', 'rb') as f:\n",
    "#     reviews, label, word2idx, idx2word = pickle.load(f)\n",
    "\n",
    "# # pre-trained embedding layer의 W를 읽어온다.\n",
    "# with open('dataset/embedding_W.pickle', 'rb') as f:\n",
    "#     We = pickle.load(f)\n",
    "\n",
    "# 학습 데이터를 vocabulary의 인덱스 (수치)로 표현한다.\n",
    "X = []\n",
    "for review in reviews:\n",
    "    X.append([word2idx[w] for w in nltk.word_tokenize(review)])\n",
    "             \n",
    "# 학습, 시험 데이터로 분리한다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, label, test_size=0.2)\n",
    "\n",
    "\n",
    "vocab_size = len(word2idx) + 1  # 사전의 크기\n",
    "max_length = 200                # 한 개 리뷰 문서의 최대 단어 길이\n",
    "   \n",
    "# 1개 리뷰 문서의 단어 개수를 max_length = 200으로 맞춘다.\n",
    "# 200개 보다 작으면 padding = 0을 추가하고, 200개 보다 크면 뒷 부분을 자른다.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "# Deep Learning architecture parameters\n",
    "batch_size = 512\n",
    "embedding_dims = 64\n",
    "num_kernels = 260        # convolution filter 개수\n",
    "kernel_size = 3          # convolution filter size\n",
    "hidden_dims = 300\n",
    "epochs = 10\n",
    "\n",
    "xInput = Input(batch_shape = (None, max_length))\n",
    "emb = Embedding(vocab_size, embedding_dims)(xInput)\n",
    "emb = Dropout(0.5)(emb)\n",
    "conv = Conv1D(num_kernels, kernel_size, padding='valid', activation='relu', strides=1)(emb)\n",
    "conv = GlobalMaxPooling1D()(conv)\n",
    "ffn = Dense(hidden_dims)(conv)\n",
    "ffn = Dropout(0.5)(ffn)\n",
    "ffn = Activation('relu')(ffn)\n",
    "ffn = Dense(1)(ffn)\n",
    "yOutput = Activation('sigmoid')(ffn)\n",
    "\n",
    "model = Model(xInput, yOutput)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "print(model.summary())\n",
    "\n",
    "# SKNS에서 학습한 We를 적용한다. (pre-trained)\n",
    "model.layers[1].set_weights(We)\n",
    "\n",
    "# 학습한다. (fine-tune)\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 batch_size=batch_size, \n",
    "                 epochs=epochs,\n",
    "                 validation_data = (x_tewest, y_test))\n",
    "\n",
    "# Loss history를 그린다\n",
    "plt.plot(hist.history['loss'], label='Train loss')\n",
    "plt.plot(hist.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# 성능 확인\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print (\"Test accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
