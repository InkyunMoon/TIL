{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[데이터사이언스스쿨](https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/)\n",
    "  \n",
    "[텐서플로와 머신러닝으로 시작하는 자연어 처리](https://book.naver.com/bookdb/book_detail.nhn?bid=14488487)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어를 표현하는 방법\n",
    "\n",
    "## 개요\n",
    "\n",
    "### 원 핫 인코딩\n",
    "> 각 단어의 인덱스를 정한 후 각 단어의 벡터를 그 단어에 해당하는 인덱스의 값을 1로 표현하는 방법\n",
    "- 문제점\n",
    "    - 1. 희소문제: 단어 벡터의 크기가 너무 커지면 공간을 많이 사용한다.(공간에 비해 실 사용 값은 1 하나 뿐이므로 비효율적)\n",
    "    - 2. 값 자체는 단순히 단어가 무엇인지만 알려주고, 의미나 특성 등이 전혀 표현되지 않는다.\n",
    "    \n",
    "위와 같은 문제들을 해결하기 위해서 분포 가설(Distributed hypothesis)을 기반으로 한 방법들이 제안되었다.\n",
    "> 분포가설이란, '같은 문맥의 단어. 즉, 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다'는 의미\n",
    "\n",
    "### 카운트 기반 방법\n",
    "> 어떤 글의 문맥 안에 단어가 동시에 등장하는 횟수를 세는 방법\n",
    "- 동시 등장 횟수를 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만든다.\n",
    "\n",
    "- 장점\n",
    "    - 빠르다. 단어 벡터가 많아져도 시간이 크게 증가하지 않는다.\n",
    "    - 데이터가 많을 경우, 단어가 잘 표현되어 효율적이다.\n",
    "\n",
    "### 예측 방법\n",
    "> 신경망 구조 혹은 모델을 사용해서 특정 문맥에서 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식  \n",
    "\n",
    "\n",
    "[워드 임베딩](https://github.com/InkyunMoon/TIL/blob/master/4.%20NLP/Word%20Embedding.ipynb)\n",
    "- 장점\n",
    "    - 카운트 기반 방법보다 단어 간의 유사도를 잘 측정한다.\n",
    "    - 단어들의 복잡한 특징까지도 잘 잡아낸다.\n",
    "    - 만들어진 단어 벡터는 서로에게 유의미한 관계를 측정할 수 있다.\n",
    "        - 엄마-아빠 사이의 거리와 여자-남자 사이의 거리가 비슷하게 나온다.\n",
    "        \n",
    "        \n",
    "보통의 경우 예측 기반 방법의 성능이 좋아서 예측 기반 방법을 사용하며, 두 방법을 모두 포함하는 Glove라는 방법도 있다.\n",
    "그러나, 항상 좋은 성능을 내는 방법이 있는 것은 아니므로 각 방법간 차이점을 숙지하고 상황에 맞게 사용해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hashing trick을 이용한 word embedding\n",
    "- Vocabulary의 크기를 미리 지정하고 hash함수 혹은 md5함수를 이용하여 단어들을 hash table에 대응시키는 방식\n",
    "    - hash table : 넓은 범위의 키 값을 작은 메모리 영역에 대응시키는 방법. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['너 오늘 이뻐 보인다.',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "labels = [[1],[0],[1],[1],[0],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['너 오늘 이뻐 보인다.',\n",
    "          '나는 오늘 기분이 더러워',\n",
    "          '나 좋은 일이 생겼어',\n",
    "          '아 오늘 진짜 짜증나',\n",
    "          '환상적인데, 정말 좋은거 같아']\n",
    "labels = [[1],[0],[1],[1],[0],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 4 1 6]\n",
      " [6 4 8 1]\n",
      " [4 8 5 1]\n",
      " [6 4 6 3]\n",
      " [9 4 7 1]]\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10\n",
    "sequences = [hashing_trick(s, VOCAB_SIZE) for s in samples]\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer 내부 출력층의 개수\n",
    "EMB_SIZE = 8\n",
    "\n",
    "xInput = Input(batch_shape=(None,sequences.shape[1]))\n",
    "embed_input = Embedding(input_dim = VOCAB_SIZE + 1, output_dim = EMB_SIZE)(xInput)\n",
    "embed_input1 = tf.reduce_mean(embed_input, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = Dense(128, activation = tf.nn.relu)(embed_input1)\n",
    "output = Dense(1, activation = 'sigmoid')(hidden_layer)\n",
    "model = Model(xInput, output)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 5\n  y sizes: 6\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-225b2bdebe72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 5\n  y sizes: 6\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "model.fit(sequences, labels, epochs=100)\n",
    "pred = model.predict(sequences)\n",
    "print(np.round(pred,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CountVectorizer & Co-occurence matrix\n",
    "\n",
    "### CountVectorizer\n",
    "> 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW인코딩한 벡터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 2, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second second documner.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer의 인수들\n",
    "- stop_words\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "- analyzer\n",
    "- token_pattern\n",
    "- tokenizer\n",
    "- ngram_range\n",
    "- max_df\n",
    "- min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop_words\n",
    "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t2\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 5)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit_transform(corpus)\n",
    "print(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'성진과': 3, '창욱은': 6, '야구장에': 4, '갔다': 0, '태균은': 7, '도서관에': 2, '공부를': 1, '좋아한다': 5}\n"
     ]
    }
   ],
   "source": [
    "docs = ['성진과 창욱은 야구장에 갔다',\n",
    "       '성진과 태균은 도서관에 갔다',\n",
    "       '성진과 창욱은 공부를 좋아한다']\n",
    "\n",
    "count_model = CountVectorizer(ngram_range=(1,1))\n",
    "x = count_model.fit_transform(docs)\n",
    "vocab_size = count_model.vocabulary_\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "# Compact Sparse Row(CSR) format\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 1]\n",
      " [0 1 0 1 0 1 1 0]]\n",
      "\n",
      "[[1 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x.toarray()) # DTM형태\n",
    "print()\n",
    "print(x.T.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurence matrix를 만든다.\n",
    "xc = x.T * x\n",
    "xc.setdiag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 2 1 0 1 1]\n",
      " [0 0 0 1 0 1 1 0]\n",
      " [1 0 0 1 0 0 0 1]\n",
      " [2 1 1 0 1 1 2 1]\n",
      " [1 0 0 1 0 0 1 0]\n",
      " [0 1 0 1 0 0 1 0]\n",
      " [1 1 0 2 1 1 0 0]\n",
      " [1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(xc.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer의 ngram_range인자를 (1,2)로 설정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'성진과': 5, '창욱은': 11, '야구장에': 8, '갔다': 0, '성진과 창욱은': 6, '창욱은 야구장에': 13, '야구장에 갔다': 9, '태균은': 14, '도서관에': 3, '성진과 태균은': 7, '태균은 도서관에': 15, '도서관에 갔다': 4, '공부를': 1, '좋아한다': 10, '창욱은 공부를': 12, '공부를 좋아한다': 2}\n"
     ]
    }
   ],
   "source": [
    "count_model = CountVectorizer(ngram_range=(1,2))\n",
    "x = count_model.fit_transform(docs)\n",
    "\n",
    "print(count_model.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 2 1 1 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1]\n",
      " [2 1 1 1 1 0 2 1 1 1 1 2 1 1 1 1]\n",
      " [1 1 1 0 0 2 0 0 1 1 1 2 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0]\n",
      " [1 1 1 0 0 2 2 0 1 1 1 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0]\n",
      " [1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "xc = x.T * x\n",
    "xc.setdiag(0)\n",
    "print(xc.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 구한 Co-occurence matrix를 SVD로 분해해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33  0.25 -0.2   0.28 -0.   -0.79  0.3   0.   -0.   -0.    0.   -0.\n",
      "  -0.    0.   -0.   -0.  ]\n",
      " [-0.19 -0.22  0.32  0.07  0.   -0.13 -0.21 -0.07 -0.5  -0.22  0.49 -0.03\n",
      "  -0.17  0.27 -0.33  0.01]\n",
      " [-0.19 -0.22  0.32  0.07  0.   -0.13 -0.21  0.08  0.46 -0.17 -0.11  0.1\n",
      "   0.02 -0.02 -0.01  0.69]\n",
      " [-0.15  0.35  0.13  0.09 -0.    0.14 -0.09  0.12 -0.51  0.17 -0.56 -0.03\n",
      "  -0.2  -0.16 -0.21  0.27]\n",
      " [-0.15  0.35  0.13  0.09  0.    0.14 -0.09  0.16 -0.15  0.32  0.52  0.08\n",
      "   0.21 -0.19  0.49  0.23]\n",
      " [-0.47  0.06  0.04 -0.87  0.   -0.04  0.16 -0.    0.   -0.   -0.    0.\n",
      "  -0.    0.   -0.    0.  ]\n",
      " [-0.37 -0.24 -0.06  0.24  0.71  0.32  0.38  0.   -0.    0.   -0.   -0.\n",
      "  -0.   -0.   -0.   -0.  ]\n",
      " [-0.15  0.35  0.13  0.09  0.    0.14 -0.09 -0.33  0.17 -0.54  0.09  0.11\n",
      "  -0.16 -0.52 -0.05 -0.21]\n",
      " [-0.21 -0.06 -0.39  0.02 -0.    0.05 -0.36  0.58 -0.11 -0.44 -0.06 -0.15\n",
      "   0.29 -0.05  0.06 -0.1 ]\n",
      " [-0.21 -0.06 -0.39  0.02  0.    0.05 -0.36 -0.44  0.04  0.12 -0.   -0.49\n",
      "  -0.34  0.09  0.27  0.14]\n",
      " [-0.19 -0.22  0.32  0.07 -0.   -0.13 -0.21  0.33  0.24  0.41  0.   -0.23\n",
      "  -0.25 -0.34 -0.08 -0.43]\n",
      " [-0.37 -0.24 -0.06  0.24 -0.71  0.32  0.38 -0.    0.   -0.   -0.   -0.\n",
      "  -0.    0.   -0.   -0.  ]\n",
      " [-0.19 -0.22  0.32  0.07 -0.   -0.13 -0.21 -0.34 -0.2  -0.02 -0.38  0.16\n",
      "   0.41  0.08  0.42 -0.28]\n",
      " [-0.21 -0.06 -0.39  0.02 -0.    0.05 -0.36 -0.14  0.07  0.32  0.06  0.65\n",
      "   0.06 -0.04 -0.33 -0.05]\n",
      " [-0.15  0.35  0.13  0.09 -0.    0.14 -0.09  0.22  0.24 -0.08 -0.08  0.24\n",
      "  -0.37  0.63  0.21 -0.22]\n",
      " [-0.15  0.35  0.13  0.09  0.    0.14 -0.09 -0.16  0.25  0.14  0.03 -0.39\n",
      "   0.53  0.25 -0.44 -0.07]] \n",
      "\n",
      "[9.24 4.88 2.74 2.65 2.   1.81 1.4  1.   1.   1.   1.   1.   1.   1.\n",
      " 1.   1.  ] \n",
      "\n",
      "[[-0.33 -0.19 -0.19 -0.15 -0.15 -0.47 -0.37 -0.15 -0.21 -0.21 -0.19 -0.37\n",
      "  -0.19 -0.21 -0.15 -0.15]\n",
      " [ 0.25 -0.22 -0.22  0.35  0.35  0.06 -0.24  0.35 -0.06 -0.06 -0.22 -0.24\n",
      "  -0.22 -0.06  0.35  0.35]\n",
      " [-0.2   0.32  0.32  0.13  0.13  0.04 -0.06  0.13 -0.39 -0.39  0.32 -0.06\n",
      "   0.32 -0.39  0.13  0.13]\n",
      " [-0.28 -0.07 -0.07 -0.09 -0.09  0.87 -0.24 -0.09 -0.02 -0.02 -0.07 -0.24\n",
      "  -0.07 -0.02 -0.09 -0.09]\n",
      " [-0.   -0.    0.    0.    0.   -0.   -0.71 -0.    0.   -0.   -0.    0.71\n",
      "   0.    0.    0.   -0.  ]\n",
      " [ 0.79  0.13  0.13 -0.14 -0.14  0.04 -0.32 -0.14 -0.05 -0.05  0.13 -0.32\n",
      "   0.13 -0.05 -0.14 -0.14]\n",
      " [-0.3   0.21  0.21  0.09  0.09 -0.16 -0.38  0.09  0.36  0.36  0.21 -0.38\n",
      "   0.21  0.36  0.09  0.09]\n",
      " [-0.    0.07 -0.08 -0.12 -0.16  0.   -0.    0.33 -0.58  0.44 -0.33  0.\n",
      "   0.34  0.14 -0.22  0.16]\n",
      " [-0.    0.5  -0.46  0.51  0.15  0.    0.   -0.17  0.11 -0.04 -0.24  0.\n",
      "   0.2  -0.07 -0.24 -0.25]\n",
      " [ 0.    0.22  0.17 -0.17 -0.32 -0.   -0.    0.54  0.44 -0.12 -0.41  0.\n",
      "   0.02 -0.32  0.08 -0.14]\n",
      " [-0.   -0.49  0.11  0.56 -0.52 -0.    0.   -0.09  0.06  0.   -0.    0.\n",
      "   0.38 -0.06  0.08 -0.03]\n",
      " [-0.    0.03 -0.1   0.03 -0.08 -0.   -0.   -0.11  0.15  0.49  0.23  0.\n",
      "  -0.16 -0.65 -0.24  0.39]\n",
      " [-0.    0.17 -0.02  0.2  -0.21 -0.    0.    0.16 -0.29  0.34  0.25 -0.\n",
      "  -0.41 -0.06  0.37 -0.53]\n",
      " [-0.   -0.27  0.02  0.16  0.19 -0.   -0.    0.52  0.05 -0.09  0.34 -0.\n",
      "  -0.08  0.04 -0.63 -0.25]\n",
      " [-0.    0.33  0.01  0.21 -0.49 -0.    0.    0.05 -0.06 -0.27  0.08  0.\n",
      "  -0.42  0.33 -0.21  0.44]\n",
      " [ 0.   -0.01 -0.69 -0.27 -0.23 -0.    0.    0.21  0.1  -0.14  0.43  0.\n",
      "   0.28  0.05  0.22  0.07]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = xc.toarray()\n",
    "U, S ,VT = np.linalg.svd(C, full_matrices= True)\n",
    "print(np.round(U,2), '\\n') # m*m\n",
    "print(np.round(S,2), '\\n') # m*n\n",
    "print(np.round(VT,2), '\\n') # n*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.24 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   4.88 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   2.74 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   2.65 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.81 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   1.4  0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  1.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# S가 대각선 성분만 리턴되었으므로 정방행렬로 바꾼다.\n",
    "\n",
    "s = np.diag(S)\n",
    "print(np.round(s,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD의 차원을 축소한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components = 4, n_iter = 7)\n",
    "D = svd.fit_transform(xc.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.24239861, 4.88199345, 2.73513827, 2.65397905])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = D / svd.singular_values_\n",
    "S = np.diag(svd.singular_values_)\n",
    "VT = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.06865075,  1.23647381,  0.56000039, -0.74876096],\n",
       "       [ 1.78329007, -1.07282969, -0.86432257, -0.18455785],\n",
       "       [ 1.78329007, -1.07282969, -0.86432253, -0.18455804],\n",
       "       [ 1.40748621,  1.71823003, -0.36323929, -0.23273073],\n",
       "       [ 1.40748621,  1.71823003, -0.36323939, -0.23273021],\n",
       "       [ 4.309953  ,  0.27899382, -0.10055292,  2.29734383],\n",
       "       [ 3.41102723, -1.14902613,  0.16473944, -0.6269283 ],\n",
       "       [ 1.40748621,  1.71823003, -0.36323925, -0.23273041],\n",
       "       [ 1.96076728, -0.27154282,  1.0731673 , -0.06332774],\n",
       "       [ 1.96076728, -0.27154282,  1.07316733, -0.06332764],\n",
       "       [ 1.78329007, -1.07282969, -0.86432253, -0.18455797],\n",
       "       [ 3.41102723, -1.14902613,  0.16473944, -0.6269283 ],\n",
       "       [ 1.78329007, -1.07282969, -0.86432254, -0.18455825],\n",
       "       [ 1.96076728, -0.27154282,  1.07316726, -0.06332803],\n",
       "       [ 1.40748621,  1.71823003, -0.36323928, -0.23273017],\n",
       "       [ 1.40748621,  1.71823003, -0.36323922, -0.23273022]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C를 4개의 차원으로 축소\n",
    "U@S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
