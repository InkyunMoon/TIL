{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "import collections\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import pickle\n",
    "\n",
    "# 전처리\n",
    "def MyPreprocessing(text):\n",
    "    text2 = \"\".join([\" \" if ch in string.punctuation else ch for ch in text])\n",
    "    tokens = nltk.word_tokenize(text2)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text\n",
    "\n",
    "# IMDB 데이터에 사용된 총 단어의 종류는 88,584개 (vocabulary 크기)이다.\n",
    "# 가장 많이 사용되는 6,000개 단어만 사용하고, 나머지는 OOV로 표시한다.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=6000,\n",
    "                                                      start_char=0,\n",
    "                                                      oov_char=0,\n",
    "                                                      index_from=0)\n",
    "\n",
    "# train, test 데이터를 합친다. 필요한 경우 나중에 나눠쓴다.\n",
    "text = np.hstack([x_train, x_test])\n",
    "label = np.hstack([y_train, y_test])\n",
    "\n",
    "# vocabulary를 가져온다.\n",
    "word2idx = imdb.get_word_index()\n",
    "idx2word = dict((v,k) for k,v in word2idx.items())\n",
    "\n",
    "# start_char와 oov_char는 '.'으로 표시해 둔다. 나중에 전처리 과정에서 제거된다.\n",
    "idx2word[0] = '.'\n",
    "\n",
    "# 숫자로 표시된 x_train을 실제 단어로 변환한다.\n",
    "def decode(review):\n",
    "    x = [idx2word[s] for s in review]\n",
    "    return ' '.join(x)\n",
    "\n",
    "# 리뷰 문서를 전처리한다.\n",
    "reviews = []\n",
    "for i, review in enumerate(text):\n",
    "    reviews.append(MyPreprocessing(decode(review)))\n",
    "    if i % 100 == 0: print(i)\n",
    "\n",
    "# 전처리된 리뷰 문서로 vocabulary를 다시 생성한다.\n",
    "counter = collections.Counter()\n",
    "for line in reviews:\n",
    "    for word in nltk.word_tokenize(line):\n",
    "        counter[word.lower()] += 1\n",
    "\n",
    "word2idx2 = {w:(i+1) for i, (w, _) in enumerate(counter.most_common())}\n",
    "idx2word2 = {v:k for k,v in word2idx2.items()}\n",
    "\n",
    "# 전처리된 리뷰 문서와 vocabulary를 저장한다.\n",
    "with open('data/preprocessed_review.pickle', 'wb') as f:\n",
    "    pickle.dump([reviews, label, word2idx2, idx2word2], f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
