{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 전처리에 관하여\n",
    "[자연어 처리 쿡북 with 파이썬을 정리한 자료입니다.](https://book.naver.com/bookdb/book_detail.nhn?bid=14451255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "- 문서 또는 긴 문자열을 다루거나 처리하려는 경우, 가장 먼저 할 일은 단어와 구두점으로 구분하는 것이다. 이를 토큰화라고 한다.\n",
    "- 언어 처리 작업에서 처리할 수 있는 최소 단위는 토큰이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commader', 'od', 'the', 'Armies', 'of', 'the', 'North', ',', 'General', 'of', 'the', 'Felix', 'Legions', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', ',', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "raw = 'My name is Maximus Decimus Meridius, commader od the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius, Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next'\n",
    "\n",
    "tokenizer = word_tokenize(raw)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk모듈의 word_tokenize는 토큰화의 정의를 따르기 때문에 널리 쓰인다.\n",
    "- 이 외에도 여러 Tokenizer가 있다.\n",
    "    - LineTokenizer, SpaceTokenizer, Tweettokenizer 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stammer\n",
    "> Stemming이란 어간을 추출하는 것이다. 어간이란 접미사가 없는 단어의 기본형이다.\n",
    "\n",
    "### 1) Porter Stemmer\n",
    "1. 객체를 초기화한다.\n",
    "2. 어간을 입력 테스트의 모든 토큰에 적용한 다음 결과를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'maximu', 'decimu', 'meridiu', ',', 'commad', 'od', 'the', 'armi', 'of', 'the', 'north', ',', 'gener', 'of', 'the', 'felix', 'legion', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'marcu', 'aureliu', ',', 'father', 'to', 'a', 'murder', 'son', ',', 'husband', 'to', 'a', 'murder', 'wife', '.', 'and', 'I', 'will', 'have', 'my', 'vengeanc', ',', 'in', 'thi', 'life', 'or', 'the', 'next']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어간을 삭제하다보니 명사 뒤의 s도 삭제해 버린 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Lancaster Stemmer\n",
    "- 포터 스테머보다 제거할 접미사가 더 많이 포함되어 있으므로 오류가 발생하기 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'nam', 'is', 'maxim', 'decim', 'meridi', ',', 'commad', 'od', 'the', 'army', 'of', 'the', 'nor', ',', 'gen', 'of', 'the', 'felix', 'leg', 'and', 'loy', 'serv', 'to', 'the', 'tru', 'emp', ',', 'marc', 'aureli', ',', 'fath', 'to', 'a', 'murd', 'son', ',', 'husband', 'to', 'a', 'murd', 'wif', '.', 'and', 'i', 'wil', 'hav', 'my', 'veng', ',', 'in', 'thi', 'lif', 'or', 'the', 'next']\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lStems = [lancaster.stem(t) for t in tokens]\n",
    "print(lStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 두 스태머를 비교해보면 Lancaster stemmer가 더 greedy 한 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer\n",
    "> lemma란 어휘 목록 표제어를 의미한다. 즉, 단어의 기본형이다.\n",
    "\n",
    "Stemmer와 달리 접미사를 제거하거나 대체함으로써 얻는 어간과 달리 사전과 일치하는 형태이다. 사전과 매칭하기 때문에, Stemming보다 느리다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commader', 'od', 'the', 'Armies', 'of', 'the', 'North', ',', 'General', 'of', 'the', 'Felix', 'Legions', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', ',', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stemmer와 달리, 명사 끝의 s를 제거하지 않았다. 그러나 명사가 아닌 단어(legions, armies 등)에 대해서는 제거하고 대치했다.\n",
    "- 'murdered'와 같이 아무 작업도 하지 않는데서 기인한 오류가 있는것을 확인할 수 있다. 완벽하지는 않더라도, Stemmer보다는 기본형을 얻는데 훨씬 뛰어나다.\n",
    "\n",
    "- WordNetLemmatizer는 사전에서 결과 단어를 찾을 수 있는 경우에만 접미사를 제거한다. 따라서 속도가 느리다.\n",
    "- 대문자로 시작하는 단어를 대명사로 인식해 처리하지 않는다.\n",
    "    - 이 경우, 입력 문자열을 소문자로 변환한 뒤 시행하면 해결된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.15\n",
    "\n",
    "## nltk.ConditionalFreqDist\n",
    "\n",
    ">빈도수를 시각화해주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 5 conditions>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "celebrity = [ ('Madonna','Singer'),('Brad Pitt','Actor'), ('Rain','Singer'), ('Gisele Bundchen','Model'), ('Rain','Actor'), ('Yun ju,Jang','Model'),('Yun ju,Jang', 'Actor') ]\n",
    "  \n",
    "cf = nltk.ConditionalFreqDist(celebrity)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Actor  Model Singer \n",
      "      Brad Pitt      1      0      0 \n",
      "Gisele Bundchen      0      1      0 \n",
      "        Madonna      0      0      1 \n",
      "           Rain      1      0      1 \n",
      "    Yun ju,Jang      1      1      0 \n"
     ]
    }
   ],
   "source": [
    "cf.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk.FreqDist \n",
    "\n",
    "[데이터사이언스스쿨](https://datascienceschool.net/view-notebook/8895b16a141749a9bb381007d52721c1/), [위키독스](https://wikidocs.net/31766)\n",
    "\n",
    ">FreqDist클래스는 문서에 사용된 토큰의 사용빈도 정보를 담는 클래스이다. \n",
    ">\n",
    ">Text클래스의 vocab메서드로 추출할 수 있다.\n",
    "\n",
    "- 단어를 key, 출현 빈도를 value로 가지는 사전 자료형과 유사하다. \n",
    "\n",
    "- most_common()은 상위 빈도수를 가진 단어만을 **튜플**로 리턴한다. 리턴된 값은 value를 기준으로 내림차순 정렬되어있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-245257a4d73b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
